"""
å˜æ›´è®°å½•ï¼ˆæ‰‹åŠ¨ç»´æŠ¤ï¼‰:
- 2026-02-09 03:29 ä¿ç•™å¥åº·æ£€æŸ¥è°ƒåº¦å¹¶ç»Ÿä¸€ä»»åŠ¡é€šçŸ¥/ç»“æœä¸ŠæŠ¥
- 2026-02-09 04:10 å¥åº·æ£€æŸ¥å¢åŠ èµ·å§‹æ—¶é—´å¹¶åœ¨å‰ç«¯æ˜¾ç¤ºé¢„è®¡ä¸‹æ¬¡æ£€æŸ¥
- 2026-02-09 04:40 æ¥å…¥ PushPlus å¹¶å¢åŠ å¾®ä¿¡é€šçŸ¥é…ç½®å…¥å£
"""

from flask import Flask, render_template, request, jsonify
from jinja2 import Environment, TemplateSyntaxError
import requests
import json
import urllib.parse
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from datetime import datetime, timedelta, timezone
import traceback
import schedule
import time
import threading
import os
import hashlib
import re
import random
import builtins


def timestamped_print(*args, **kwargs):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if args:
        args = (f"[{ts}] {args[0]}",) + args[1:]
    else:
        args = (f"[{ts}]",)
    return builtins.print(*args, **kwargs)


print = timestamped_print

HEALTH_CHECK_NEXT_RUN = None


class StateSampler:
    """æŒ‰ç§’èšåˆ state åˆ†å¸ƒï¼ˆä»…è®¡æ•°ï¼‰å¹¶ç»™å‡º locked æ¨èã€‚"""

    def __init__(self):
        self._lock = threading.Lock()
        self._bucket = {}
        self._max_buckets = 300

    def ingest(self, raw_list):
        now_sec = int(time.time())
        counts = {}
        for place in raw_list or []:
            for slot in place.get('projectInfo', []) or []:
                try:
                    key = int(slot.get('state'))
                except Exception:
                    key = -999
                counts[key] = counts.get(key, 0) + 1
        with self._lock:
            self._bucket[now_sec] = counts
            stale_before = now_sec - self._max_buckets
            for ts in list(self._bucket.keys()):
                if ts < stale_before:
                    self._bucket.pop(ts, None)

    def snapshot(self):
        with self._lock:
            data = dict(self._bucket)
        merged = {}
        for v in data.values():
            for s, c in v.items():
                merged[s] = merged.get(s, 0) + int(c)

        available_count = merged.get(1, 0)
        locked_recommend = []
        for state, cnt in sorted(merged.items(), key=lambda x: (-x[1], x[0])):
            if state in (1, 4, -999):
                continue
            if cnt <= 0:
                continue
            if cnt >= max(5, int(available_count * 0.05)):
                locked_recommend.append(state)

        return {
            'seconds': len(data),
            'states': merged,
            'recommended_locked_states': locked_recommend,
        }


STATE_SAMPLER = StateSampler()


def normalize_time_str(value):
    if not value:
        return None
    if isinstance(value, str):
        value = value.strip()
        try:
            dt = datetime.strptime(value, "%H:%M")
            return dt.strftime("%H:%M")
        except ValueError:
            return None
    return None

# å®šæœŸå¥åº·æ£€æŸ¥çš„å‡½æ•°
def health_check():
    """
    å®šæœŸæ£€æŸ¥è·å–åœºåœ°çŠ¶æ€æ˜¯å¦æ­£å¸¸ï¼Œå¹¶å‘é€çŸ­ä¿¡é€šçŸ¥ã€‚
    """
    phones = CONFIG.get('notification_phones') or []
    pushplus_tokens = CONFIG.get('pushplus_tokens') or []
    today = datetime.now().strftime("%Y-%m-%d")
    matrix_res = client.get_matrix(today)
    if "error" in matrix_res:
        err_msg = matrix_res["error"]
        log(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: è·å–åœºåœ°çŠ¶æ€å¼‚å¸¸: {err_msg}")
        if phones:
            task_manager.send_notification(f"âš ï¸ å¥åº·æ£€æŸ¥å¤±è´¥ï¼šè·å–åœºåœ°çŠ¶æ€å¼‚å¸¸({err_msg})", phones=phones)
        if pushplus_tokens:
            task_manager.send_wechat_notification(
                f"âš ï¸ å¥åº·æ£€æŸ¥å¤±è´¥ï¼šè·å–åœºåœ°çŠ¶æ€å¼‚å¸¸({err_msg})",
                tokens=pushplus_tokens,
            )
    else:
        booking_probe = client.check_booking_auth_probe()
        if booking_probe.get('ok') and booking_probe.get('unknown'):
            log(f"âœ… å¥åº·æ£€æŸ¥é€šè¿‡ï¼šåœºåœ°çŠ¶æ€è·å–æ­£å¸¸ï¼›âš ï¸ ä¸‹å•é“¾è·¯ä»…å®Œæˆæ¢æµ‹ï¼Œç»“æœæœªç¡®è®¤( {booking_probe.get('msg')} )")
        elif booking_probe.get('ok'):
            log("âœ… å¥åº·æ£€æŸ¥é€šè¿‡ï¼šåœºåœ°çŠ¶æ€è·å–æ­£å¸¸ï¼›ä¸‹å•é‰´æƒæ¢æµ‹æœªè§æ˜æ˜¾å¼‚å¸¸")
        else:
            if booking_probe.get('unknown'):
                log(f"âœ… å¥åº·æ£€æŸ¥é€šè¿‡ï¼šåœºåœ°çŠ¶æ€è·å–æ­£å¸¸ï¼›âš ï¸ ä¸‹å•é“¾è·¯æ¢æµ‹å¼‚å¸¸/æœªçŸ¥( {booking_probe.get('msg')} )")
            else:
                log(f"âš ï¸ å¥åº·æ£€æŸ¥ï¼šæŸ¥è¯¢æ­£å¸¸ï¼Œä½†ä¸‹å•é“¾è·¯ç–‘ä¼¼é‰´æƒå¼‚å¸¸( {booking_probe.get('msg')} )")

# æ¯éš”ä¸€æ®µæ—¶é—´æ‰§è¡Œå¥åº·æ£€æŸ¥
def schedule_health_check():
    """
    å®šæ—¶ä»»åŠ¡ï¼šæŒ‰ç…§é…ç½®çš„é—´éš”æ—¶é—´è¿è¡Œå¥åº·æ£€æŸ¥ã€‚
    """
    # æ¸…ç†å·²æœ‰çš„å¥åº·æ£€æŸ¥ä»»åŠ¡ï¼Œé¿å…é‡å¤è°ƒåº¦
    schedule.clear("health_check")

    if not CONFIG.get('health_check_enabled', True):
        print("ğŸ›‘ å¥åº·æ£€æŸ¥å·²å…³é—­ï¼Œä¸å®‰æ’å®šæ—¶ä»»åŠ¡ã€‚")
        return

    check_interval = CONFIG.get('health_check_interval_min', 30)
    try:
        check_interval = float(check_interval)
    except (TypeError, ValueError):
        check_interval = 30.0
    if check_interval < 1:
        check_interval = 1
    start_time = CONFIG.get('health_check_start_time', '00:00')
    start_time = normalize_time_str(start_time) or '00:00'

    def compute_next_run():
        now = datetime.now()
        start_dt = datetime.strptime(
            f"{now.strftime('%Y-%m-%d')} {start_time}", "%Y-%m-%d %H:%M"
        )
        if now <= start_dt:
            return start_dt
        elapsed = (now - start_dt).total_seconds() / 60.0
        steps = int(elapsed // check_interval) + 1
        return start_dt + timedelta(minutes=steps * check_interval)

    def health_check_tick():
        global HEALTH_CHECK_NEXT_RUN
        if HEALTH_CHECK_NEXT_RUN is None:
            HEALTH_CHECK_NEXT_RUN = compute_next_run()
        if datetime.now() >= HEALTH_CHECK_NEXT_RUN:
            health_check()
            HEALTH_CHECK_NEXT_RUN = HEALTH_CHECK_NEXT_RUN + timedelta(minutes=check_interval)

    global HEALTH_CHECK_NEXT_RUN
    HEALTH_CHECK_NEXT_RUN = compute_next_run()
    schedule.every(1).minutes.do(health_check_tick).tag("health_check")
    print(
        f"ğŸ“… å¥åº·æ£€æŸ¥å·²å®‰æ’ï¼Œèµ·å§‹æ—¶é—´ {start_time}ï¼Œæ¯ {check_interval} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡."
    )


app = Flask(__name__)

# ================= é…ç½® =================
CONFIG = {
    "auth": {
        "token": "oy9Aj1fKpR3Yxwd6iV7VIlg3Vo-A", # è¯·ç¡®ä¿æœ‰æ•ˆ
        "cookie": "JSESSIONID=FFE6C0633F33D9CE71354D0D1110AC0D",
        "card_index": "0873612446",
        "card_st_id": "289", 
        "shop_num": "1001"
    },
    "sms": {
        "user": "18600291931",
        "api_key": "6127d94d28a04c06a8f61b70eac79cc3"
    },
    "notification_phones": [],
    "pushplus_tokens": [],
    "retry_interval": 1.0,
    "aggressive_retry_interval": 1.0,
    "batch_retry_times": 2,
    "batch_retry_interval": 0.5,
    "submit_batch_size": 3,
    "initial_submit_batch_size": 2,
    "submit_timeout_seconds": 4.0,
    "submit_split_retry_times": 1,
    "batch_min_interval": 0.8,
    "order_query_timeout_seconds": 2.5,
    "order_query_max_pages": 2,
    "post_submit_orders_join_timeout_seconds": 1.2,
    "post_submit_verify_orders_on_matrix_partial_only": True,
    "post_submit_orders_sync_fallback": False,
    "post_submit_verify_pending_retry_seconds": 0.35,
    "post_submit_treat_verify_timeout_as_retry": True,
    "refill_window_seconds": 8.0,
    "locked_retry_interval": 1.0,  # âœ… æ–°å¢ï¼šé”å®šçŠ¶æ€é‡è¯•é—´éš”(ç§’)
    "locked_max_seconds": 60,  # âœ… æ–°å¢ï¼šé”å®šçŠ¶æ€æœ€å¤šåˆ· N ç§’
    "locked_state_values": [2, 3, 5, 6],  # æ¥å£ state è½åœ¨è¿™äº›å€¼æ—¶è§†ä¸ºâ€œé”å®š/æš‚ä¸å¯ä¸‹å•â€
    "open_retry_seconds": 30,  # âœ… æ–°å¢ï¼šå·²å¼€æ”¾æ— ç»„åˆæ—¶ç»§ç»­é‡è¯•çª—å£(ç§’)
    "matrix_timeout_seconds": 3.0,  # é«˜å³°æŸ¥è¯¢è¶…æ—¶(ç§’)ï¼Œå»ºè®®çŸ­è¶…æ—¶+é«˜é¢‘é‡è¯•
    "stop_on_none_stage_without_refill": False,  # pipeline é˜¶æ®µç»“æŸä¸”æ—  refill æ—¶æ˜¯å¦ç«‹å³ç»“æŸ
    # ğŸ” æ–°å¢ï¼šå‡­è¯å¥åº·æ£€æŸ¥
    "health_check_enabled": True,      # æ˜¯å¦å¼€å¯è‡ªåŠ¨å¥åº·æ£€æŸ¥
    "health_check_interval_min": 30.0, # æ£€æŸ¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    "health_check_start_time": "00:00", # èµ·å§‹æ—¶é—´ (HH:MM)
    "verbose_logs": False,  # æ˜¯å¦æ‰“å°é«˜é¢‘è°ƒè¯•æ—¥å¿—
    "same_time_precheck_limit": 0,  # åŒæ—¶æ®µé¢„æ£€ä¸Šé™ï¼›<=0 è¡¨ç¤ºå…³é—­é¢„æ£€
    "biz_fail_cooldown_seconds": 15.0,  # pipeline ä¸­ä¸šåŠ¡å¤±è´¥ç»„åˆå†·å´æ—¶é—´
    "preselect_enabled": True,
    "preselect_ttl_seconds": 2.0,
    "preselect_only_before_first_submit": True,
}


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_TEMPLATE_FILE = os.path.join(BASE_DIR, "config.json")
CONFIG_FILE = CONFIG_TEMPLATE_FILE
LOG_BUFFER = []
MAX_LOG_SIZE = 500
MAX_TARGET_COUNT = 9
REFILL_TASKS_FILE = os.path.join(BASE_DIR, "refill_tasks.json")
TASK_RUN_METRICS_FILE = os.path.join(BASE_DIR, "task_run_metrics.json")
_TASK_RUN_METRICS_LOCK = threading.Lock()


def _percentile(sorted_values, p):
    if not sorted_values:
        return None
    if len(sorted_values) == 1:
        return float(sorted_values[0])
    idx = int(round((len(sorted_values) - 1) * float(p)))
    idx = max(0, min(len(sorted_values) - 1, idx))
    return float(sorted_values[idx])


def append_task_run_metric(record, keep_last=200):
    try:
        with _TASK_RUN_METRICS_LOCK:
            old = []
            if os.path.exists(TASK_RUN_METRICS_FILE):
                try:
                    with open(TASK_RUN_METRICS_FILE, 'r', encoding='utf-8') as f:
                        old = json.load(f) or []
                except Exception:
                    old = []
            if not isinstance(old, list):
                old = []
            old.append(record)
            old = old[-max(10, int(keep_last or 200)):]
            with open(TASK_RUN_METRICS_FILE, 'w', encoding='utf-8') as f:
                json.dump(old, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä»»åŠ¡æŒ‡æ ‡å†™å…¥å¤±è´¥: {e}")

def log(msg):
    """è®°å½•æ—¥å¿—åˆ°å†…å­˜ç¼“å†²åŒºå’Œæ§åˆ¶å°"""
    print(msg)
    timestamp = datetime.now().strftime("%H:%M:%S")
    LOG_BUFFER.append(f"[{timestamp}] {msg}")
    if len(LOG_BUFFER) > MAX_LOG_SIZE:
        LOG_BUFFER.pop(0)


def is_verbose_logs_enabled():
    return bool(CONFIG.get("verbose_logs", False))

if os.path.exists(CONFIG_FILE):
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            saved = json.load(f)
            if 'notification_phones' in saved:
                CONFIG['notification_phones'] = saved['notification_phones']
            if 'pushplus_tokens' in saved:
                CONFIG['pushplus_tokens'] = saved['pushplus_tokens']
            if 'retry_interval' in saved:
                CONFIG['retry_interval'] = saved['retry_interval']
            if 'aggressive_retry_interval' in saved:
                CONFIG['aggressive_retry_interval'] = saved['aggressive_retry_interval']
            if 'batch_retry_times' in saved:
                CONFIG['batch_retry_times'] = saved['batch_retry_times']
            if 'batch_retry_interval' in saved:
                CONFIG['batch_retry_interval'] = saved['batch_retry_interval']
            if 'submit_batch_size' in saved:
                CONFIG['submit_batch_size'] = saved['submit_batch_size']
            if 'initial_submit_batch_size' in saved:
                try:
                    CONFIG['initial_submit_batch_size'] = max(1, min(9, int(saved['initial_submit_batch_size'])))
                except Exception:
                    pass
            if 'submit_timeout_seconds' in saved:
                try:
                    CONFIG['submit_timeout_seconds'] = max(0.5, float(saved['submit_timeout_seconds']))
                except Exception:
                    pass
            if 'submit_split_retry_times' in saved:
                try:
                    CONFIG['submit_split_retry_times'] = max(0, min(3, int(saved['submit_split_retry_times'])))
                except Exception:
                    pass
            if 'batch_min_interval' in saved:
                CONFIG['batch_min_interval'] = saved['batch_min_interval']
            if 'order_query_timeout_seconds' in saved:
                try:
                    CONFIG['order_query_timeout_seconds'] = max(0.5, float(saved['order_query_timeout_seconds']))
                except Exception:
                    pass
            if 'order_query_max_pages' in saved:
                try:
                    CONFIG['order_query_max_pages'] = max(1, min(10, int(saved['order_query_max_pages'])))
                except Exception:
                    pass
            if 'post_submit_orders_join_timeout_seconds' in saved:
                try:
                    CONFIG['post_submit_orders_join_timeout_seconds'] = max(0.1, float(saved['post_submit_orders_join_timeout_seconds']))
                except Exception:
                    pass
            if 'post_submit_verify_orders_on_matrix_partial_only' in saved:
                CONFIG['post_submit_verify_orders_on_matrix_partial_only'] = bool(saved['post_submit_verify_orders_on_matrix_partial_only'])
            if 'post_submit_orders_sync_fallback' in saved:
                CONFIG['post_submit_orders_sync_fallback'] = bool(saved['post_submit_orders_sync_fallback'])
            if 'post_submit_verify_pending_retry_seconds' in saved:
                try:
                    CONFIG['post_submit_verify_pending_retry_seconds'] = max(0.05, float(saved['post_submit_verify_pending_retry_seconds']))
                except Exception:
                    pass
            if 'post_submit_treat_verify_timeout_as_retry' in saved:
                CONFIG['post_submit_treat_verify_timeout_as_retry'] = bool(saved['post_submit_treat_verify_timeout_as_retry'])
            if 'refill_window_seconds' in saved:
                CONFIG['refill_window_seconds'] = saved['refill_window_seconds']
            # âœ… æ–°å¢ï¼šé”å®šé‡è¯•çš„ä¸¤ä¸ªé…ç½®
            if 'locked_retry_interval' in saved:
                CONFIG['locked_retry_interval'] = saved['locked_retry_interval']
            if 'locked_max_seconds' in saved:
                CONFIG['locked_max_seconds'] = saved['locked_max_seconds']
            if 'locked_state_values' in saved and isinstance(saved['locked_state_values'], list):
                parsed_locked_states = []
                for v in saved['locked_state_values']:
                    try:
                        parsed_locked_states.append(int(v))
                    except Exception:
                        continue
                if parsed_locked_states:
                    CONFIG['locked_state_values'] = parsed_locked_states
            if 'open_retry_seconds' in saved:
                CONFIG['open_retry_seconds'] = saved['open_retry_seconds']
            if 'matrix_timeout_seconds' in saved:
                try:
                    CONFIG['matrix_timeout_seconds'] = max(0.5, float(saved['matrix_timeout_seconds']))
                except Exception:
                    pass
            if 'stop_on_none_stage_without_refill' in saved:
                CONFIG['stop_on_none_stage_without_refill'] = bool(saved['stop_on_none_stage_without_refill'])
            if 'health_check_enabled' in saved:
                CONFIG['health_check_enabled'] = saved['health_check_enabled']
            if 'health_check_interval_min' in saved:
                CONFIG['health_check_interval_min'] = saved['health_check_interval_min']
            if 'health_check_start_time' in saved:
                CONFIG['health_check_start_time'] = normalize_time_str(saved['health_check_start_time']) or CONFIG['health_check_start_time']
            if 'verbose_logs' in saved:
                CONFIG['verbose_logs'] = bool(saved['verbose_logs'])
            if 'preselect_enabled' in saved:
                CONFIG['preselect_enabled'] = bool(saved['preselect_enabled'])
            if 'preselect_ttl_seconds' in saved:
                try:
                    CONFIG['preselect_ttl_seconds'] = max(0.2, float(saved['preselect_ttl_seconds']))
                except Exception:
                    pass
            if 'preselect_only_before_first_submit' in saved:
                CONFIG['preselect_only_before_first_submit'] = bool(saved['preselect_only_before_first_submit'])
            if 'same_time_precheck_limit' in saved:
                try:
                    CONFIG['same_time_precheck_limit'] = int(saved['same_time_precheck_limit'])
                except Exception:
                    pass
            if 'biz_fail_cooldown_seconds' in saved:
                try:
                    CONFIG['biz_fail_cooldown_seconds'] = max(1.0, float(saved['biz_fail_cooldown_seconds']))
                except Exception:
                    pass
            if 'auth' in saved:
                # è¦†ç›–é»˜è®¤çš„ auth é…ç½®
                CONFIG['auth'].update(saved['auth'])
    except Exception as e:
        print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")

TASKS_TEMPLATE_FILE = os.path.join(BASE_DIR, "tasks.json")
TASKS_FILE = TASKS_TEMPLATE_FILE

class ApiClient:
    def __init__(self):
        self.host = "gymvip.bfsu.edu.cn"
        self.headers = {
            "Host": self.host,
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090a13) UnifiedPCWindowsWechat(0xf254162e) XWEB/18151 Flue",
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": f"https://{self.host}",
            "Referer": f"https://{self.host}/easyserp/index.html",
        }
        cookie = str(CONFIG["auth"].get("cookie", "")).strip()
        if cookie:
            self.headers["Cookie"] = cookie
        self.token = CONFIG["auth"]["token"]
        self.session = requests.Session()
        self.server_time_offset_seconds = 0.0
        self._matrix_cache = {}
        self._matrix_cache_window_s = 0.12
        self._matrix_cache_lock = threading.Lock()

    def _update_server_time_offset(self, resp, started_at, ended_at):
        date_header = (resp.headers or {}).get("Date") if resp is not None else None
        if not date_header:
            return
        try:
            from email.utils import parsedate_to_datetime
            server_dt = parsedate_to_datetime(date_header)
            if server_dt.tzinfo is None:
                server_dt = server_dt.replace(tzinfo=timezone.utc)
            server_ts = server_dt.timestamp()
            midpoint = (started_at + ended_at) / 2.0
            self.server_time_offset_seconds = server_ts - midpoint
        except Exception:
            return

    def get_aligned_now(self):
        return datetime.now() + timedelta(seconds=float(self.server_time_offset_seconds or 0.0))

    def check_token(self):
        # ç®€å•è¯·æ±‚ä¸€æ¬¡æ¥å£ï¼Œçœ‹æ˜¯å¦è¿”å› token å¤±æ•ˆç›¸å…³çš„é”™è¯¯
        # è¿™é‡Œç”¨è·å–çŸ©é˜µæ¥å£æµ‹è¯•ï¼Œå› ä¸ºå®ƒåªè¯»ä¸”è½»é‡
        today = datetime.now().strftime("%Y-%m-%d")
        res = self.get_matrix(today)
        
        # å‡è®¾æ¥å£è¿”å› msg åŒ…å« "token" æˆ– "ç™»å½•" å­—æ ·ä»£è¡¨å¤±æ•ˆ
        # å…·ä½“æ ¹æ®å®é™…æŠ“åŒ…é”™è¯¯ç è°ƒæ•´
        if "error" in res:
            err = res["error"]
            # æ‰©å±•å…³é”®è¯ï¼šå¢åŠ  "å¤±æ•ˆ", "å‡­è¯", "-1"
            if any(k in err.lower() for k in ["token", "ç™»å½•", "session", "å¤±æ•ˆ", "å‡­è¯", "-1"]):
                return False, err
        return True, "Valid"

    def check_booking_auth_probe(self):
        """
        å°è¯•ç”¨â€œæ— æ•ˆä¸šåŠ¡å‚æ•°â€çš„è½»é‡è¯·æ±‚æ¢æµ‹ reservationPlace é‰´æƒé“¾è·¯ã€‚
        è¯´æ˜ï¼šæ­¤æ¢æµ‹ä¸æäº¤æœ‰æ•ˆåœºæ¬¡ï¼Œä¸ä¼šäº§ç”ŸçœŸå®è®¢å•ï¼›
        ä»…ç”¨äºåŒºåˆ†â€œé‰´æƒå¤±è´¥â€å’Œâ€œä¸šåŠ¡å‚æ•°é”™è¯¯/æœªçŸ¥â€ã€‚
        """
        url = f"https://{self.host}/easyserpClient/place/reservationPlace"
        probe_body = (
            f"token={self.token}&"
            f"shopNum={CONFIG['auth']['shop_num']}&"
            f"fieldinfo=%5B%5D&"
            f"cardStId={CONFIG['auth'].get('card_st_id', '')}&"
            f"oldTotal=0.00&"
            f"cardPayType=0&"
            f"type=&"
            f"offerId=&"
            f"offerType=&"
            f"total=0.00&"
            f"premerother=&"
            f"cardIndex={CONFIG['auth'].get('card_index', '')}"
        )

        try:
            resp = self.session.post(url, headers=self.headers, data=probe_body, timeout=10, verify=False)
            text = (resp.text or '').strip()
            data = None
            try:
                data = resp.json()
            except Exception:
                data = None

            msg_raw = ''
            if isinstance(data, dict):
                msg_raw = str(data.get('msg') or data.get('data') or '')
            if not msg_raw:
                msg_raw = text[:160]
            msg_l = msg_raw.lower()

            auth_keywords = ['token', 'session', 'ç™»å½•', 'å¤±æ•ˆ', 'å‡­è¯', '-1', 'æœªç™»å½•']
            if any(k in msg_l for k in auth_keywords):
                return {'ok': False, 'unknown': False, 'msg': msg_raw}

            # èƒ½èµ°åˆ°è¿™é‡Œé€šå¸¸è¯´æ˜æ¥å£å¯è¾¾ä¸”æœªè¢«ç›´æ¥é‰´æƒæ‹¦æˆªï¼›
            # ä½†ç”±äºæ˜¯æ— æ•ˆä¸šåŠ¡å‚æ•°æ¢æµ‹ï¼Œä¸èƒ½è§†ä¸ºâ€œä¸‹å•ä¸€å®šæˆåŠŸâ€ã€‚
            return {'ok': True, 'unknown': True, 'msg': f"æ¢æµ‹å“åº”: {msg_raw}"}
        except Exception as e:
            return {'ok': False, 'unknown': True, 'msg': f"æ¢æµ‹å¼‚å¸¸: {e}"}

    def get_place_orders(self, page_size=20, max_pages=6, timeout_s=10):
        """è·å–æˆ‘çš„åœºåœ°è®¢å•åˆ—è¡¨ï¼ˆç”¨äºè¯†åˆ« mine çŠ¶æ€ï¼‰ã€‚"""
        url = f"https://{self.host}/easyserpClient/place/getPlaceOrder"
        all_orders = []

        for page_no in range(max_pages):
            params = {
                "pageNo": page_no,
                "pageSize": page_size,
                "shopNum": CONFIG["auth"]["shop_num"],
                "token": self.token,
            }
            try:
                resp = self.session.get(
                    url,
                    headers=self.headers,
                    params=params,
                    timeout=max(0.5, float(timeout_s or 10)),
                    verify=False,
                )
                data = resp.json()
            except Exception as e:
                return {"error": f"è·å–è®¢å•å¤±è´¥: {e}"}

            if not isinstance(data, dict):
                return {"error": f"è®¢å•æ¥å£è¿”å›æ ¼å¼é”™è¯¯: {data}"}
            if data.get("msg") != "success":
                return {"error": f"è®¢å•æ¥å£è¿”å›å¼‚å¸¸: {data.get('msg')}"}

            page_items = data.get("data") or []
            if not isinstance(page_items, list):
                page_items = []

            all_orders.extend(page_items)
            if len(page_items) < page_size:
                break

        return {"data": all_orders}

    def _extract_mine_slots(self, orders, target_date):
        """æŠŠè®¢å•åˆ—è¡¨è½¬æ¢ä¸º mine æ ¼å­é›†åˆï¼Œæ ¼å¼: {(place, HH:MM)}ã€‚"""
        mine_slots = set()
        for order in orders:
            if str(order.get("showStatus", "")) != "0":
                continue
            if str(order.get("prestatus", "")).strip() in ("å–æ¶ˆ", "å·²å–æ¶ˆ"):
                continue

            arr = order.get("jsonArray") or []
            if not isinstance(arr, list):
                continue

            for seg in arr:
                if str(seg.get("reversionDate", "")).strip() != target_date:
                    continue

                site_name = str(seg.get("siteName", ""))
                m = re.search(r"(\d+)", site_name)
                if not m:
                    continue
                place = m.group(1)

                start = str(seg.get("start", "")).strip()
                end = str(seg.get("end", "")).strip()
                try:
                    start_dt = datetime.strptime(start, "%H:%M:%S")
                    end_dt = datetime.strptime(end, "%H:%M:%S")
                except ValueError:
                    continue

                cur = start_dt
                while cur < end_dt:
                    mine_slots.add((place, cur.strftime("%H:%M")))
                    cur += timedelta(hours=1)

        return mine_slots

    def extract_mine_slots_by_date(self, orders):
        """æŒ‰æ—¥æœŸèšåˆ mine æ ¼å­ï¼Œè¿”å› {date: [{'place':'7','time':'20:00'}]}ã€‚"""
        grouped = {}
        for order in orders or []:
            if str(order.get("showStatus", "")) != "0":
                continue
            if str(order.get("prestatus", "")).strip() in ("å–æ¶ˆ", "å·²å–æ¶ˆ"):
                continue
            arr = order.get("jsonArray") or []
            if not isinstance(arr, list):
                continue
            for seg in arr:
                date_str = str(seg.get("reversionDate", "")).strip()
                if not date_str:
                    continue
                site_name = str(seg.get("siteName", ""))
                m = re.search(r"(\d+)", site_name)
                if not m:
                    continue
                place = m.group(1)
                start = str(seg.get("start", "")).strip()
                end = str(seg.get("end", "")).strip()
                try:
                    start_dt = datetime.strptime(start, "%H:%M:%S")
                    end_dt = datetime.strptime(end, "%H:%M:%S")
                except ValueError:
                    continue
                cur = start_dt
                while cur < end_dt:
                    grouped.setdefault(date_str, set()).add((place, cur.strftime("%H:%M")))
                    cur += timedelta(hours=1)

        result = {}
        for d, slots in grouped.items():
            result[d] = [
                {"place": p, "time": t}
                for p, t in sorted(slots, key=lambda x: (int(x[0]) if str(x[0]).isdigit() else 999, x[1]))
            ]
        return result

    def get_matrix(self, date_str, include_mine_overlay=True):
        cache_key = (str(date_str or ''), bool(include_mine_overlay))
        now_ts = time.time()
        with self._matrix_cache_lock:
            cache_hit = self._matrix_cache.get(cache_key)
        if cache_hit and (now_ts - float(cache_hit.get('ts', 0.0))) <= float(self._matrix_cache_window_s):
            try:
                return json.loads(json.dumps(cache_hit.get('data')))
            except Exception:
                return cache_hit.get('data')

        url = f"https://{self.host}/easyserpClient/place/getPlaceInfoByShortName"
        params = {
            "shopNum": CONFIG["auth"]["shop_num"],
            "dateymd": date_str,
            "shortName": "ymq",
            "token": self.token
        }
        try:
            # æŠ¢ç¥¨é«˜å³°æœŸé‡‡ç”¨çŸ­è¶…æ—¶ï¼Œé¿å…å•æ¬¡è¯·æ±‚å¡ä½åæ‰é»„é‡‘çª—å£ï¼›é…åˆä¸Šå±‚é«˜é¢‘é‡è¯•ã€‚
            started_at = time.time()
            matrix_timeout = max(0.5, float(CONFIG.get('matrix_timeout_seconds', 3.0) or 3.0))
            resp = self.session.get(url, headers=self.headers, params=params, timeout=matrix_timeout, verify=False)
            ended_at = time.time()
            self._update_server_time_offset(resp, started_at, ended_at)

            try:
                data = resp.json()
            except json.JSONDecodeError:
                # æœåŠ¡å™¨å¯èƒ½è¿”å›äº† HTML é”™è¯¯é¡µæˆ–ç©ºå†…å®¹
                print(f"âŒ [åŸå§‹å“åº”] éJSONæ ¼å¼: {resp.text[:100]}...")
                return {"error": "æœåŠ¡å™¨è¿”å›æ— æ•ˆæ•°æ®(å¯èƒ½æ˜¯å´©äº†)"}
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ data æ˜¯å­—å…¸
            if not isinstance(data, dict):
                print(f"âŒ [APIå“åº”å¼‚å¸¸] å“åº”ä¸æ˜¯å­—å…¸: {type(data)} - {data}")
                # ç‰¹æ®Šå¤„ç† -1 (é€šå¸¸ä»£è¡¨ Session/Token å¤±æ•ˆ)
                if data == -1 or str(data) == "-1":
                    return {"error": "ä¼šè¯å¤±æ•ˆ(è¿”å›-1)ï¼Œè¯·æ›´æ–°Tokenï¼ˆå¿…è¦ï¼‰ä¸Cookieï¼ˆå¯é€‰ï¼‰"}
                return {"error": f"APIè¿”å›æ ¼å¼é”™è¯¯: {data}"}

            if data.get("msg") != "success":
                return {"error": data.get("msg")}
            
            raw_data = data.get('data')
            if isinstance(raw_data, str):
                try: raw_list = json.loads(raw_data)
                except: return {"error": "JSONè§£æå¤±è´¥"}
            else:
                raw_list = raw_data
                
            if isinstance(raw_list, dict):
                if 'placeArray' in raw_list:
                    raw_list = raw_list['placeArray']
                else:
                    return {"error": "æ— æ³•æ‰¾åˆ°åœºåœ°åˆ—è¡¨"}

            STATE_SAMPLER.ingest(raw_list)

            matrix = {}
            all_times = set()
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ‰“å°å‰å‡ ä¸ªæ•°æ®çš„çŠ¶æ€å€¼ï¼Œä»¥ä¾¿åˆ†æâ€œå…¨çº¢â€åŸå› 
            debug_states = []

            locked_state_values = set()
            for raw_state in CONFIG.get('locked_state_values', [2, 3, 5, 6]):
                try:
                    locked_state_values.add(int(raw_state))
                except Exception:
                    continue
            if not locked_state_values:
                locked_state_values = {6}

            for place in raw_list:
                p_name = place['projectName']['shortname'] 
                p_num = p_name.replace('ymq', '').replace('mdb', '')
                
                status_map = {}
                for slot in place['projectInfo']:
                    t = slot['starttime']
                    s = slot['state']
                    all_times.add(t)
                    
                    if len(debug_states) < 5:
                        debug_states.append(f"{p_num}å·{t}={s}")

                    # 1=å¯ç”¨, å…¶ä»–=å ç”¨
                    # æ ¹æ®è°ƒè¯•æ—¥å¿—ä¿®æ­£ï¼š
                    # state=4: ä¼¼ä¹æ˜¯â€œå·²å ç”¨â€æˆ–â€œé”å®šâ€ (å…¨çº¢æ—¶å…¨æ˜¯4)
                    # state=6: ä¼¼ä¹æ˜¯â€œæœªå¼€æ”¾â€æˆ–â€œæœªæ¥â€ (å‘¨äº”å…¨æ˜¯6)
                    # state=1: å¶å°”å‡ºç°ï¼Œåº”è¯¥æ˜¯â€œå¯ç”¨â€
                    # state=0: æœªçŸ¥
                    
                    # å…³é”®ä¿®æ”¹ï¼š
                    # æ—¢ç„¶ç”¨æˆ·ç›®çš„æ˜¯â€œæå‰é€‰ä¸­ç„¶åå‡†æ—¶ä¸‹å•â€ï¼Œæˆ‘ä»¬éœ€è¦æŠŠâ€œæœªå¼€æ”¾â€çš„çŠ¶æ€ä¹Ÿè§†ä¸ºâ€œå¯ç”¨(available)â€
                    # è¿™æ ·ç”¨æˆ·åœ¨å‰ç«¯å°±èƒ½é€‰ä¸­å¹¶æ·»åŠ åˆ°æ„¿æœ›å•äº†ã€‚
                    # å‡è®¾ 6 æ˜¯æœªå¼€æ”¾ä½†å°†æ¥ä¼šå¼€æ”¾ã€‚
                    # å‡è®¾ 4 æ˜¯å·²ç»è¢«åˆ«äººè®¢äº†ï¼ˆä¸å¯é€‰ï¼‰ã€‚
                    # å‡è®¾ 1 æ˜¯å½“å‰å°±èƒ½ä¹°ï¼ˆå¯ç”¨ï¼‰ã€‚
                    
                    # ç­–ç•¥ï¼šåªè¦ä¸æ˜¯æ˜ç¡®çš„â€œå·²é¢„è®¢(4?)â€ï¼Œéƒ½ç®— availableï¼Ÿ
                    # æˆ–è€…æ›´ç²¾ç¡®ç‚¹ï¼š1(å¯ç”¨) å’Œ 6(æœªå¼€æ”¾) éƒ½ç®— availableã€‚
                    # æš‚æ—¶æŠŠ 6 ä¹ŸåŠ è¿›å»ã€‚

                    try:
                        state_int = int(s)
                    except Exception:
                        state_int = -999

                    if state_int == 1:
                        # çœŸæ­£å¯ä»¥ä¸‹å•
                        status_map[t] = "available"
                    elif state_int in locked_state_values:
                        # é”å®š/æš‚ä¸å¯ä¸‹å•ï¼šç»§ç»­èµ° locked è½®è¯¢ï¼Œä¸æå‰æ”¾å¼ƒ
                        status_map[t] = "locked"
                    else:
                        # å·²è¢«åˆ«äººè®¢äº† / ä¸å¯ç”¨
                        status_map[t] = "booked"

                matrix[p_num] = status_map
            
            if is_verbose_logs_enabled():
                print(f"ğŸ” [çŠ¶æ€è°ƒè¯•] å‰5ä¸ªæ ·æœ¬çŠ¶æ€: {debug_states}")

            # ç”¨æˆ‘çš„è®¢å•è¦†ç›– mine çŠ¶æ€ï¼ˆä»… showStatus=0 ä¸”éå–æ¶ˆè®¢å•ï¼‰
            mine_overlay_ok = False
            mine_overlay_error = ""
            mine_slots_count = 0

            if include_mine_overlay:
                orders_res = self.get_place_orders()
                if "error" not in orders_res:
                    mine_overlay_ok = True
                    mine_slots = self._extract_mine_slots(orders_res.get("data", []), date_str)
                    mine_slots_count = len(mine_slots)
                    for p, t in mine_slots:
                        if p in matrix and t in matrix[p]:
                            matrix[p][t] = "mine"
                    if mine_slots and is_verbose_logs_enabled():
                        print(f"ğŸ”µ [mineè¦†ç›–] æ—¥æœŸ{date_str} å…±æ ‡è®° {len(mine_slots)} ä¸ªmineæ ¼å­")
                else:
                    mine_overlay_error = str(orders_res.get('error') or '')
                    if is_verbose_logs_enabled():
                        print(f"âš ï¸ [mineè¦†ç›–] è®¢å•æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡mineçŠ¶æ€: {mine_overlay_error}")
            else:
                mine_overlay_error = "é¦–è½®åŠ é€Ÿæ¨¡å¼ï¼šè·³è¿‡mineè¦†ç›–"

            sorted_places = sorted(matrix.keys(), key=lambda x: int(x) if x.isdigit() else 999)
            sorted_times = sorted(list(all_times))

            result = {
                "places": sorted_places,
                "times": sorted_times,
                "matrix": matrix,
                "meta": {
                    "mine_overlay_ok": mine_overlay_ok,
                    "mine_slots_count": mine_slots_count,
                    "mine_overlay_error": mine_overlay_error,
                }
            }
            with self._matrix_cache_lock:
                self._matrix_cache[cache_key] = {'ts': time.time(), 'data': result}
                if len(self._matrix_cache) > 8:
                    oldest = min(self._matrix_cache.keys(), key=lambda k: self._matrix_cache[k].get('ts', 0.0))
                    self._matrix_cache.pop(oldest, None)
            return result
            
        except Exception as e:
            return {"error": str(e)}

    def submit_order(self, date_str, selected_items):
        """
        æäº¤é¢„è®¢è®¢å•ã€‚
        å…³é”®ä¿®æ­£ï¼šä¸å†å•çº¯ä¾èµ– reservationPlace è¿”å›çš„ "msg":"success"ï¼Œ
        è€Œæ˜¯æäº¤å®Œæˆåé‡æ–°æ‹‰å–çŸ©é˜µï¼Œç¡®è®¤é€‰ä¸­åœºæ¬¡çš„çŠ¶æ€æ˜¯å¦ä» available å˜ä¸º bookedã€‚
        """
        url = f"https://{self.host}/easyserpClient/place/reservationPlace"

        results = []
        try:
            degrade_batch_size = int(CONFIG.get("submit_batch_size", 3))
        except Exception:
            degrade_batch_size = 3
        degrade_batch_size = max(1, min(9, degrade_batch_size))
        configured_initial_batch_size = int(CONFIG.get("initial_submit_batch_size", CONFIG.get("submit_batch_size", 3)) or 3)
        initial_batch_size = max(1, min(9, configured_initial_batch_size))
        batch_retry_times = int(CONFIG.get("batch_retry_times", 2))
        batch_retry_interval = float(CONFIG.get("batch_retry_interval", CONFIG.get("retry_interval", 0.5)))
        batch_min_interval = float(CONFIG.get("batch_min_interval", 0.8))
        refill_window_seconds = float(CONFIG.get("refill_window_seconds", 8.0))
        submit_timeout_seconds = max(0.5, float(CONFIG.get("submit_timeout_seconds", 4.0) or 4.0))
        submit_split_retry_times = max(0, min(3, int(CONFIG.get("submit_split_retry_times", 1) or 1)))

        print(
            f"ğŸ§­ [æ‰¹æ¬¡ç­–ç•¥] é¦–æ‰¹=æŒ‰é…ç½® initial_submit_batch_sizeâ†’{initial_batch_size}ï¼›"
            f"é™çº§=æŒ‰é…ç½® submit_batch_sizeâ†’{degrade_batch_size}ï¼›æœ¬æ¬¡é€‰æ‹©={len(selected_items)}"
        )
        print(
            f"â±ï¸ [æäº¤è¶…æ—¶] submit_timeout={submit_timeout_seconds}s, split_retry_times={submit_split_retry_times}"
        )

        def normalize_fail_message(msg):
            text = str(msg or "").strip()
            if not text:
                return "ä¸‹å•å¤±è´¥(ç©ºå“åº”)"
            lower = text.lower()
            if "<html" in lower and "404" in lower:
                return "ä¸‹å•æ¥å£æš‚æ—¶ä¸å¯ç”¨(404)"
            if "404 not found" in lower:
                return "ä¸‹å•æ¥å£æš‚æ—¶ä¸å¯ç”¨(404)"
            if "502" in lower or "503" in lower or "504" in lower:
                return "ä¸‹å•æ¥å£æš‚æ—¶ä¸å¯ç”¨(ç½‘å…³å¼‚å¸¸)"
            if len(text) > 180:
                return text[:180] + "..."
            return text

        def is_retryable_fail(msg):
            text = str(msg or "").lower()
            keywords = [
                "æ“ä½œè¿‡å¿«", "ç¨åé‡è¯•", "è¯·æ±‚è¿‡äºé¢‘ç¹", "too fast", "é¢‘ç¹",
                "404 not found", "nginx", "bad gateway", "service unavailable",
                "502", "503", "504", "timeout", "timed out", "connection reset",
                "max retries exceeded", "temporarily unavailable", "non-json", "éjson",
                "æš‚æ—¶ä¸å¯ç”¨", "ç½‘å…³å¼‚å¸¸", "ä¸‹å•æ¥å£æš‚æ—¶ä¸å¯ç”¨", "ç©ºå“åº”",
            ]
            return any(k in text for k in keywords)

        def should_degrade(msg):
            text = str(msg or "")
            rule_keywords = [
                "è§„åˆ™",
                "æœ€å¤šé¢„çº¦3ä¸ª",
                "æœ€å¤šé¢„çº¦",
                "ä¸Šé™",
            ]
            return is_retryable_fail(text) or any(k in text for k in rule_keywords)

        def filter_still_available(items):
            try:
                verify = self.get_matrix(date_str, include_mine_overlay=False)
                if not isinstance(verify, dict) or verify.get("error"):
                    return list(items)
                matrix = verify.get("matrix") or {}
                remain = []
                for it in items:
                    p = str(it.get("place"))
                    t = it.get("time")
                    if matrix.get(p, {}).get(t) == "available":
                        remain.append(it)
                return remain
            except Exception:
                return list(items)

        submit_items = list(selected_items or [])
        preblocked_items = []
        same_time_limit = int(CONFIG.get("same_time_precheck_limit", 0) or 0)
        if same_time_limit > 0:
            try:
                verify = self.get_matrix(date_str)
                if isinstance(verify, dict) and not verify.get("error"):
                    matrix = verify.get("matrix") or {}
                    mine_by_time = {}
                    for row in matrix.values():
                        if not isinstance(row, dict):
                            continue
                        for t, state in row.items():
                            if state == "mine":
                                mine_by_time[t] = mine_by_time.get(t, 0) + 1

                    planned_by_time = {}
                    allowed_items = []
                    for it in submit_items:
                        t = it.get("time")
                        quota = max(0, same_time_limit - mine_by_time.get(t, 0))
                        used = planned_by_time.get(t, 0)
                        if used < quota:
                            allowed_items.append(it)
                            planned_by_time[t] = used + 1
                        else:
                            preblocked_items.append(it)

                    if preblocked_items:
                        print(
                            f"âš ï¸ [åŒæ—¶æ®µä¸Šé™é¢„æ£€] è§¦å‘ä¸Šé™{same_time_limit}ï¼Œ"
                            f"æœ¬è½®è·³è¿‡ {len(preblocked_items)} é¡¹: {preblocked_items}"
                        )
                    submit_items = allowed_items
            except Exception as e:
                print(f"âš ï¸ [åŒæ—¶æ®µä¸Šé™é¢„æ£€] é¢„æ£€å¼‚å¸¸ï¼ŒæŒ‰åŸå§‹é€‰æ‹©æäº¤: {e}")
        else:
            if is_verbose_logs_enabled():
                print("âš¡ [åŒæ—¶æ®µä¸Šé™é¢„æ£€] å·²å…³é—­ï¼ˆsame_time_precheck_limit<=0ï¼‰")

        # é¦–è½®æäº¤ï¼šæŒ‰â€œæœ¬æ¬¡é€‰æ‹©æ•°é‡â€è‡ªé€‚åº”åˆ†æ‰¹
        for i in range(0, len(submit_items), initial_batch_size):
            batch = submit_items[i:i + initial_batch_size]
            print(f"ğŸ“¦ æ­£åœ¨æäº¤åˆ†æ‰¹è®¢å• ({i // initial_batch_size + 1}): {batch}")

            field_info_list = []
            total_money = 0

            for item in batch:
                p_num = item["place"]
                start = item["time"]
                # è®¡ç®—ç»“æŸæ—¶é—´ & æŒ‰å¼€å§‹æ—¶é—´å†³å®šä»·æ ¼
                try:
                    st_obj = datetime.strptime(start, "%H:%M")
                    et_obj = st_obj + timedelta(hours=1)
                    end = et_obj.strftime("%H:%M")
                    # ç®€å•ä»·æ ¼è§„åˆ™ï¼š14:00 ä¹‹å‰ 80 å…ƒï¼Œä¹‹å 100 å…ƒ
                    # å¯¹åº”æŠ“åŒ…ä¸­çš„ oldMoney åˆ†å¸ƒï¼ˆ10â€“13 ç‚¹ä¸º 80ï¼Œ14 ç‚¹ä»¥åä¸º 100ï¼‰
                    if st_obj.hour < 14:
                        price = 80
                    else:
                        price = 100
                except Exception:
                    # å¼‚å¸¸æ—¶å…œåº•ï¼šæŠŠç»“æŸæ—¶é—´å’Œä»·æ ¼éƒ½è®¾ä¸ºå¸¸è§„æ™šé—´ä»·æ ¼
                    end = "22:00"
                    price = 100

                # æ ¹æ®åœºåœ°å·åŒºåˆ†æ™®é€šåœº (1-14) å’Œæœ¨åœ°æ¿åœº (15-17)
                try:
                    p_int = int(p_num)
                except (TypeError, ValueError):
                    p_int = None

                if p_int is not None and p_int >= 15:
                    # æœ¨åœ°æ¿åœºï¼šshortname å½¢å¦‚ mdb15ï¼Œname ä¸º "æœ¨åœ°æ¿15"
                    place_short = f"mdb{p_num}"
                    place_name = f"æœ¨åœ°æ¿{p_num}"
                else:
                    # æ™®é€šç¾½æ¯›çƒåœºï¼šshortname å½¢å¦‚ ymq10ï¼Œname ä¸º "ç¾½æ¯›çƒ10"
                    place_short = f"ymq{p_num}"
                    place_name = f"ç¾½æ¯›çƒ{p_num}"

                info = {
                    "day": date_str,
                    "oldMoney": price,
                    "startTime": start,
                    "endTime": end,
                    "placeShortName": place_short,
                    "name": place_name,
                    "stageTypeShortName": "ymq",
                    "newMoney": price,
                }
                field_info_list.append(info)
                total_money += price

            info_str = urllib.parse.quote(
                json.dumps(field_info_list, separators=(",", ":"), ensure_ascii=False)
            )
            type_encoded = urllib.parse.quote("ç¾½æ¯›çƒ")

            body = (
                f"token={self.token}&"
                f"shopNum={CONFIG['auth']['shop_num']}&"
                f"fieldinfo={info_str}&"
                f"cardStId={CONFIG['auth']['card_st_id']}&"
                f"oldTotal={total_money}.00&"
                f"cardPayType=0&"
                f"type={type_encoded}&"
                f"offerId=&"
                f"offerType=&"
                f"total={total_money}.00&"
                f"premerother=&"
                f"cardIndex={CONFIG['auth']['card_index']}"
            )

            final_result = None
            for attempt in range(batch_retry_times + 1):
                try:
                    resp = self.session.post(
                        url, headers=self.headers, data=body, timeout=submit_timeout_seconds, verify=False
                    )

                    try:
                        resp_data = resp.json()
                    except ValueError:
                        resp_data = None

                    if is_verbose_logs_enabled():
                        print(
                            f"ğŸ“¨ [submit_orderè°ƒè¯•] æ‰¹æ¬¡ {i // initial_batch_size + 1} å“åº”: {resp.text}"
                        )

                    if resp_data and resp_data.get("msg") == "success":
                        final_result = {"status": "success", "batch": batch}
                        break

                    fail_msg = None
                    if isinstance(resp_data, dict):
                        fail_msg = resp_data.get("data") or resp_data.get("msg")
                    if not fail_msg:
                        fail_msg = resp.text
                    fail_msg = normalize_fail_message(fail_msg)

                    if attempt < batch_retry_times and is_retryable_fail(fail_msg):
                        sleep_s = batch_retry_interval * (2 ** attempt) + random.uniform(0, 0.25)
                        print(
                            f"â³ æ‰¹æ¬¡ {i // initial_batch_size + 1} å‘½ä¸­å¯é‡è¯•é”™è¯¯ï¼Œ"
                            f"{round(sleep_s, 2)}s åé‡è¯• ({attempt + 1}/{batch_retry_times})"
                        )
                        time.sleep(sleep_s)
                        continue

                    # å‘½ä¸­â€œå¯é‡è¯•/è§„åˆ™å¼‚å¸¸â€æ—¶ï¼ŒæŒ‰é…ç½®åˆ†æ‰¹é™çº§é‡æä¸€æ¬¡
                    if len(batch) > degrade_batch_size and should_degrade(fail_msg):
                        print(f"â†˜ï¸ æ‰¹æ¬¡ {i // initial_batch_size + 1} é™çº§é‡æ: size {len(batch)} -> {degrade_batch_size}")
                        degrade_fail = list(batch)
                        current = list(batch)
                        for split_round in range(submit_split_retry_times + 1):
                            round_fail = []
                            for j in range(0, len(current), degrade_batch_size):
                                sub = current[j:j + degrade_batch_size]
                                try:
                                    sub_field_info = []
                                    sub_total = 0
                                    for item in sub:
                                        p_num = item["place"]
                                        start = item["time"]
                                        try:
                                            st_obj = datetime.strptime(start, "%H:%M")
                                            et_obj = st_obj + timedelta(hours=1)
                                            end = et_obj.strftime("%H:%M")
                                            price = 80 if st_obj.hour < 14 else 100
                                        except Exception:
                                            end = "22:00"
                                            price = 100
                                        try:
                                            p_int = int(p_num)
                                        except (TypeError, ValueError):
                                            p_int = None
                                        if p_int is not None and p_int >= 15:
                                            place_short = f"mdb{p_num}"
                                            place_name = f"æœ¨åœ°æ¿{p_num}"
                                        else:
                                            place_short = f"ymq{p_num}"
                                            place_name = f"ç¾½æ¯›çƒ{p_num}"
                                        sub_field_info.append({
                                            "day": date_str,
                                            "oldMoney": price,
                                            "startTime": start,
                                            "endTime": end,
                                            "placeShortName": place_short,
                                            "name": place_name,
                                            "stageTypeShortName": "ymq",
                                            "newMoney": price,
                                        })
                                        sub_total += price

                                    info_str = urllib.parse.quote(json.dumps(sub_field_info, separators=(",", ":"), ensure_ascii=False))
                                    type_encoded = urllib.parse.quote("ç¾½æ¯›çƒ")
                                    sub_body = (
                                        f"token={self.token}&shopNum={CONFIG['auth']['shop_num']}&fieldinfo={info_str}&"
                                        f"cardStId={CONFIG['auth']['card_st_id']}&oldTotal={sub_total}.00&cardPayType=0&"
                                        f"type={type_encoded}&offerId=&offerType=&total={sub_total}.00&premerother=&"
                                        f"cardIndex={CONFIG['auth']['card_index']}"
                                    )
                                    sub_resp = self.session.post(url, headers=self.headers, data=sub_body, timeout=submit_timeout_seconds, verify=False)
                                    sub_data = sub_resp.json() if sub_resp.text else None
                                    if not (isinstance(sub_data, dict) and sub_data.get("msg") == "success"):
                                        round_fail.extend(sub)
                                except Exception:
                                    round_fail.extend(sub)
                                time.sleep(max(batch_min_interval, CONFIG.get("retry_interval", 0.5)))
                            degrade_fail = round_fail
                            if not degrade_fail:
                                break
                            if split_round < submit_split_retry_times:
                                current = list(degrade_fail)
                                print(f"ğŸ” [é™çº§åˆ†æ®µé‡è¯•] round={split_round + 1}/{submit_split_retry_times}, remain={len(current)}")
                                time.sleep(batch_retry_interval)

                        if not degrade_fail:
                            final_result = {"status": "success", "batch": batch}
                        else:
                            final_result = {"status": "fail", "msg": fail_msg, "batch": degrade_fail}
                        break

                    final_result = {"status": "fail", "msg": fail_msg, "batch": batch}
                    break
                except Exception as e:
                    if attempt < batch_retry_times:
                        print(
                            f"â³ æ‰¹æ¬¡ {i // initial_batch_size + 1} å¼‚å¸¸ï¼Œ{batch_retry_interval}s åé‡è¯• "
                            f"({attempt + 1}/{batch_retry_times}): {e}"
                        )
                        time.sleep(batch_retry_interval)
                        continue
                    final_result = {"status": "error", "msg": str(e), "batch": batch}
                    break

            results.append(final_result or {"status": "error", "msg": "æœªçŸ¥é”™è¯¯", "batch": batch})

            # æ‰¹æ¬¡é—´æœ€å°åœé¡¿ï¼Œé˜²æ­¢è§¦å‘â€œæ“ä½œè¿‡å¿«â€
            time.sleep(max(batch_min_interval, CONFIG.get("retry_interval", 0.5)))

        # å¯¹å¤±è´¥é¡¹åšè¡¥æï¼ˆçª—å£å†…ä»…è¡¥æä» available çš„é¡¹ï¼‰
        try:
            refill_deadline = time.time() + max(0.0, refill_window_seconds)
            while time.time() < refill_deadline:
                failed_items = []
                for r in results:
                    if r.get("status") in ("fail", "error"):
                        failed_items.extend(r.get("batch") or [])
                if not failed_items:
                    break

                still_available = filter_still_available(failed_items)
                if not still_available:
                    break

                print(f"ğŸ” [è¡¥æ] çª—å£å†…è¡¥æä»å¯ç”¨é¡¹: {still_available}")
                results = [r for r in results if r.get("status") == "success"]
                for i in range(0, len(still_available), degrade_batch_size):
                    batch = still_available[i:i + degrade_batch_size]
                    field_info_list = []
                    total_money = 0
                    for item in batch:
                        p_num = item["place"]
                        start = item["time"]
                        try:
                            st_obj = datetime.strptime(start, "%H:%M")
                            et_obj = st_obj + timedelta(hours=1)
                            end = et_obj.strftime("%H:%M")
                            price = 80 if st_obj.hour < 14 else 100
                        except Exception:
                            end = "22:00"
                            price = 100
                        try:
                            p_int = int(p_num)
                        except (TypeError, ValueError):
                            p_int = None
                        if p_int is not None and p_int >= 15:
                            place_short = f"mdb{p_num}"
                            place_name = f"æœ¨åœ°æ¿{p_num}"
                        else:
                            place_short = f"ymq{p_num}"
                            place_name = f"ç¾½æ¯›çƒ{p_num}"
                        field_info_list.append({
                            "day": date_str,
                            "oldMoney": price,
                            "startTime": start,
                            "endTime": end,
                            "placeShortName": place_short,
                            "name": place_name,
                            "stageTypeShortName": "ymq",
                            "newMoney": price,
                        })
                        total_money += price
                    info_str = urllib.parse.quote(json.dumps(field_info_list, separators=(",", ":"), ensure_ascii=False))
                    type_encoded = urllib.parse.quote("ç¾½æ¯›çƒ")
                    body = (
                        f"token={self.token}&shopNum={CONFIG['auth']['shop_num']}&fieldinfo={info_str}&"
                        f"cardStId={CONFIG['auth']['card_st_id']}&oldTotal={total_money}.00&cardPayType=0&"
                        f"type={type_encoded}&offerId=&offerType=&total={total_money}.00&premerother=&"
                        f"cardIndex={CONFIG['auth']['card_index']}"
                    )
                    try:
                        resp = self.session.post(url, headers=self.headers, data=body, timeout=10, verify=False)
                        resp_data = resp.json() if resp.text else None
                        if isinstance(resp_data, dict) and resp_data.get("msg") == "success":
                            results.append({"status": "success", "batch": batch})
                        else:
                            msg = resp_data.get("data") if isinstance(resp_data, dict) else resp.text
                            results.append({"status": "fail", "msg": msg, "batch": batch})
                    except Exception as e:
                        results.append({"status": "error", "msg": str(e), "batch": batch})
                    time.sleep(max(batch_min_interval, CONFIG.get("retry_interval", 0.5)))

                # è¡¥æåªåšä¸€è½®ï¼Œé¿å…æ— é™è½°ç‚¸
                break
        except Exception as e:
            print(f"âš ï¸ [è¡¥æ] å¤„ç†å¼‚å¸¸: {e}")

        if preblocked_items:
            results.append({
                "status": "fail",
                "msg": "åŒä¸€æ—¶é—´çš„åœºåœ°æœ€å¤šé¢„çº¦3ä¸ª(å«å·²é¢„çº¦mine)",
                "batch": preblocked_items,
            })

        # ---------- ä¸‹å•åéªŒè¯ ----------
        api_success_count = sum(1 for r in results if r.get("status") == "success")
        verify_success_count = None
        verify_success_items = []
        verify_failed_items = []
        orders_query_ok = False
        orders_query_error = ""
        try:
            verify = self.get_matrix(date_str, include_mine_overlay=False)
            orders_res = {"error": "æŒ‰é…ç½®è·³è¿‡"}
            order_timeout_s = max(0.5, float(CONFIG.get('order_query_timeout_seconds', 2.5) or 2.5))
            order_max_pages = max(1, min(10, int(CONFIG.get('order_query_max_pages', 2) or 2)))

            if isinstance(verify, dict) and not verify.get("error"):
                v_matrix = verify["matrix"]
                verify_states = []

                matrix_success_items = []
                matrix_failed_items = []
                for item in submit_items:
                    p = str(item["place"])
                    t = item["time"]
                    status = v_matrix.get(p, {}).get(t, "N/A")
                    if status in ("booked", "mine"):
                        matrix_success_items.append({"place": p, "time": t})
                    else:
                        matrix_failed_items.append({"place": p, "time": t})

                verify_orders_only_on_partial = bool(CONFIG.get('post_submit_verify_orders_on_matrix_partial_only', True))
                needs_orders_query = bool(matrix_failed_items) if verify_orders_only_on_partial else True

                if needs_orders_query:
                    orders_res = {"error": "æœªæ‰§è¡Œ"}

                    def _fetch_orders():
                        nonlocal orders_res
                        orders_res = self.get_place_orders(max_pages=order_max_pages, timeout_s=order_timeout_s)

                    t_orders = threading.Thread(target=_fetch_orders, daemon=True)
                    t_orders.start()
                    join_timeout_s = max(0.1, float(CONFIG.get('post_submit_orders_join_timeout_seconds', 1.2) or 1.2))
                    t_orders.join(timeout=join_timeout_s)
                    if isinstance(orders_res, dict) and orders_res.get("error") == "æœªæ‰§è¡Œ":
                        orders_res = {"error": f"è®¢å•æŸ¥è¯¢è¶…æ—¶(>{join_timeout_s}s)"}
                        if bool(CONFIG.get('post_submit_orders_sync_fallback', False)):
                            orders_res = self.get_place_orders(max_pages=order_max_pages, timeout_s=order_timeout_s)

                mine_slots = set()
                if "error" not in orders_res:
                    mine_slots = self._extract_mine_slots(orders_res.get("data", []), date_str)
                    orders_query_ok = True
                else:
                    orders_query_error = str(orders_res.get("error") or "")
                    print(
                        f"ğŸ§¾ [æäº¤åéªŒè¯è°ƒè¯•] è®¢å•æ‹‰å–å¤±è´¥ï¼Œmineæ ¡éªŒé™çº§ä¸ºçŸ©é˜µçŠ¶æ€: {orders_query_error}"
                    )

                for item in submit_items:
                    p = str(item["place"])
                    t = item["time"]
                    status = v_matrix.get(p, {}).get(t, "N/A")
                    mine_hit = (p, t) in mine_slots
                    verify_states.append(f"{p}å·{t}={status},mine={'Y' if mine_hit else 'N'}")

                    # è‹¥è®¢å•æŸ¥è¯¢å¯ç”¨ï¼šä¼˜å…ˆç”¨ mine å…œåº•ï¼›å¦åˆ™åªç”¨çŸ©é˜µçŠ¶æ€ï¼Œä¿è¯éªŒè¯é“¾è·¯å°½å¿«æ”¶æ•›ã€‚
                    success = mine_hit or status in ("booked", "mine") if orders_query_ok else status in ("booked", "mine")

                    if success:
                        verify_success_items.append({"place": p, "time": t})
                    else:
                        verify_failed_items.append({"place": p, "time": t})

                if preblocked_items:
                    verify_failed_items.extend(preblocked_items)
                    verify_states.extend([
                        f"{str(it.get('place'))}å·{it.get('time')}=preblocked"
                        for it in preblocked_items
                    ])

                if is_verbose_logs_enabled():
                    print(f"ğŸ§¾ [æäº¤åéªŒè¯è°ƒè¯•] é€‰ä¸­åœºæ¬¡æœ€æ–°çŠ¶æ€: {verify_states}")
                verify_success_count = len(verify_success_items)
            else:
                print(
                    f"ğŸ§¾ [æäº¤åéªŒè¯è°ƒè¯•] è·å–çŸ©é˜µå¤±è´¥: "
                    f"{verify.get('error') if isinstance(verify, dict) else verify}"
                )
        except Exception as e:
            print(f"ğŸ§¾ [æäº¤åéªŒè¯è°ƒè¯•] å¼‚å¸¸: {e}")

        # ---------- æ±‡æ€»ç»“æœ ----------
        # 1) æ¥å£è¿”å›å±‚é¢çš„æˆåŠŸæ‰¹æ¬¡æ•°

        # 2) çœŸå®å·²è¢«å ç”¨çš„åœºæ¬¡æ•°é‡ï¼ˆå¦‚æœéªŒè¯æˆåŠŸï¼‰
        verify_ok = verify_success_count is not None
        if verify_ok:
            success_count = verify_success_count
        else:
            # éªŒè¯å¤±è´¥æ—¶ä¸å†æŠŠâ€œæ¥å£ successâ€ç›´æ¥å½“ä½œæœ€ç»ˆæˆåŠŸï¼Œé¿å…è¯¯æŠ¥
            success_count = 0

        # 3) æœ¬æ¬¡è®¡åˆ’æ€»å…±å°è¯•ä¸‹å•çš„åœºæ¬¡æ•°
        total_items = len(selected_items) if selected_items else 0

        # å…¼å®¹è€é€»è¾‘ï¼šå¦‚æœ selected_items ä¸ºç©ºï¼ˆç†è®ºä¸Šä¸åº”è¯¥ï¼‰ï¼Œ
        # é€€å›åˆ°æŒ‰æ‰¹æ¬¡æ•°ç»Ÿè®¡ï¼Œé˜²æ­¢ denominator ä¸º 0ã€‚
        denominator = total_items or len(results)

        if denominator == 0:
            msg = "æ²¡æœ‰ç”Ÿæˆä»»ä½•ä¸‹å•é¡¹ç›®ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–åœºåœ°çŠ¶æ€ã€‚"
            return {"status": "fail", "msg": msg}

        cross_instance_suspected = verify_ok and api_success_count == 0 and success_count > 0

        if verify_ok and api_success_count > 0 and success_count == denominator:
            return {
                "status": "success",
                "msg": "å…¨éƒ¨ä¸‹å•æˆåŠŸ",
                "success_items": verify_success_items,
                "failed_items": verify_failed_items,
            }
        elif verify_ok and api_success_count > 0 and success_count > 0:
            return {
                "status": "partial",
                "msg": f"éƒ¨åˆ†æˆåŠŸ ({success_count}/{denominator})",
                "success_items": verify_success_items,
                "failed_items": verify_failed_items,
            }
        else:
            # æœªæ”¶åˆ°ä»»ä½•æäº¤æˆåŠŸå“åº”ï¼Œä½†æ ¡éªŒå‘½ä¸­ mineï¼Œç–‘ä¼¼å¹¶å‘å®ä¾‹ä¸‹å•å¯¼è‡´çš„â€œå½’å› ä¸²æ‰°â€
            if cross_instance_suspected:
                msg = "æ£€æµ‹åˆ°æˆ‘çš„è®¢å•å·²å ä½ï¼Œä½†æœ¬è¿›ç¨‹æäº¤æœªæ”¶åˆ° successï¼Œå¯èƒ½ç”±å¹¶å‘å®ä¾‹ä¸‹å•å¯¼è‡´ï¼›æœ¬ä»»åŠ¡æŒ‰å¤±è´¥å¤„ç†ã€‚"
            # éªŒè¯å¤±è´¥æ—¶ï¼Œå®å¯æŠ¥å¤±è´¥ä¹Ÿä¸è¯¯æŠ¥æˆåŠŸ
            elif not verify_ok and api_success_count > 0:
                msg = "ä¸‹å•æ¥å£è¿”å› successï¼Œä½†æäº¤åçŠ¶æ€éªŒè¯å¤±è´¥ï¼ˆç½‘ç»œ/æœåŠ¡æ³¢åŠ¨ï¼‰ï¼Œè¯·ä»¥å®˜æ–¹ç³»ç»Ÿä¸ºå‡†ã€‚"
            elif api_success_count > 0 and verify_success_count == 0:
                allow_verify_pending = bool(CONFIG.get("post_submit_treat_verify_timeout_as_retry", True))
                if allow_verify_pending and (not orders_query_ok):
                    return {
                        "status": "verify_pending",
                        "msg": f"æäº¤å·²è¿”å›successï¼Œä½†éªŒè¯å°šæœªæ”¶æ•›({orders_query_error or 'è®¢å•æ ¡éªŒæœªå®Œæˆ'})ï¼Œå°†å¿«é€Ÿå¤æ ¸ã€‚",
                        "success_items": verify_success_items,
                        "failed_items": verify_failed_items,
                    }
                msg = "æ¥å£è¿”å› successï¼Œä½†åœºåœ°çŠ¶æ€æœªå˜åŒ–ï¼Œè¯·åœ¨å¾®ä¿¡å°ç¨‹åºç¡®è®¤æˆ–æ£€æŸ¥å‚æ•°ã€‚"
            else:
                first_fail = results[0] if results else {"msg": "æ— æ•°æ®"}
                msg = first_fail.get("msg")
            return {
                "status": "fail",
                "msg": msg,
                "success_items": verify_success_items,
                "failed_items": verify_failed_items,
            }

    def x_submit_order_old(self, date_str, selected_items):
        pass

client = ApiClient()

# ================= ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ =================

class TaskManager:
    def __init__(self):
        self.tasks = []
        self.refill_tasks = []
        self._refill_lock = threading.Lock()
        self._refill_last_run = {}
        self._refill_notify_last_bucket = {}
        self._task_run_lock = threading.Lock()
        self._running_task_ids = set()
        self.load_tasks()
        self.load_refill_tasks()

    def _try_mark_task_running(self, task_id):
        tid = str(task_id)
        with self._task_run_lock:
            if tid in self._running_task_ids:
                return False
            self._running_task_ids.add(tid)
            return True

    def _unmark_task_running(self, task_id):
        tid = str(task_id)
        with self._task_run_lock:
            self._running_task_ids.discard(tid)

    def is_task_running(self, task_id):
        tid = str(task_id)
        with self._task_run_lock:
            return tid in self._running_task_ids

    def execute_task_with_lock(self, task):
        task_id = task.get('id')
        if task_id is not None and not self._try_mark_task_running(task_id):
            log(f"â­ï¸ [ä»»åŠ¡é”] ä»»åŠ¡{task_id}ä»åœ¨æ‰§è¡Œï¼Œè·³è¿‡æœ¬æ¬¡è§¦å‘")
            return False
        try:
            self.execute_task(task)
            return True
        finally:
            if task_id is not None:
                self._unmark_task_running(task_id)
        

    def load_refill_tasks(self):
        if os.path.exists(REFILL_TASKS_FILE):
            try:
                with open(REFILL_TASKS_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.refill_tasks = data if isinstance(data, list) else []
            except Exception:
                self.refill_tasks = []

    def save_refill_tasks(self):
        with self._refill_lock:
            with open(REFILL_TASKS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.refill_tasks, f, ensure_ascii=False, indent=2)

    def add_refill_task(self, task):
        now_ms = int(time.time() * 1000)
        task = dict(task or {})
        task['id'] = now_ms
        task['enabled'] = bool(task.get('enabled', True))
        task['interval_seconds'] = max(1.0, float(task.get('interval_seconds', 10.0) or 10.0))
        task['target_count'] = max(1, min(MAX_TARGET_COUNT, int(task.get('target_count', 1) or 1)))
        task['last_run_at'] = task.get('last_run_at')
        task['last_result'] = task.get('last_result')
        task['deadline'] = str(task.get('deadline') or '').strip()
        task['deadline_mode'] = str(task.get('deadline_mode') or 'absolute').strip() or 'absolute'
        try:
            task['deadline_before_hours'] = float(task.get('deadline_before_hours') or 2.0)
        except Exception:
            task['deadline_before_hours'] = 2.0
        task['exec_history'] = list(task.get('exec_history') or [])[-10:]
        self.refill_tasks.append(task)
        self.save_refill_tasks()
        return task

    def delete_refill_task(self, task_id):
        tid = int(task_id)
        self.refill_tasks = [t for t in self.refill_tasks if int(t.get('id', -1)) != tid]
        self._refill_last_run.pop(tid, None)
        self.save_refill_tasks()


    def update_refill_task(self, task_id, patch):
        tid = int(task_id)
        for t in self.refill_tasks:
            if int(t.get('id', -1)) != tid:
                continue
            payload = dict(patch or {})
            if 'date' in payload:
                t['date'] = str(payload.get('date') or '').strip()
            if 'target_times' in payload and isinstance(payload.get('target_times'), list):
                t['target_times'] = [str(x).strip() for x in payload.get('target_times') if str(x).strip()]
            if 'candidate_places' in payload and isinstance(payload.get('candidate_places'), list):
                t['candidate_places'] = [str(x).strip() for x in payload.get('candidate_places') if str(x).strip()]
            if 'interval_seconds' in payload:
                try:
                    t['interval_seconds'] = max(1.0, float(payload.get('interval_seconds') or 10.0))
                except Exception:
                    pass
            if 'target_count' in payload:
                try:
                    t['target_count'] = max(1, min(MAX_TARGET_COUNT, int(payload.get('target_count') or 1)))
                except Exception:
                    pass
            if 'enabled' in payload:
                t['enabled'] = bool(payload.get('enabled'))
            if 'deadline' in payload:
                t['deadline'] = str(payload.get('deadline') or '').strip()
            if 'deadline_mode' in payload:
                mode = str(payload.get('deadline_mode') or '').strip()
                t['deadline_mode'] = mode if mode in ('absolute', 'before_start') else 'absolute'
            if 'deadline_before_hours' in payload:
                try:
                    t['deadline_before_hours'] = max(0.0, float(payload.get('deadline_before_hours') or 0.0))
                except Exception:
                    pass
            self.save_refill_tasks()
            return t
        return None

    def append_refill_history(self, task, result):
        history = list(task.get('exec_history') or [])
        history.append({
            'ts': int(time.time() * 1000),
            'status': result.get('status'),
            'msg': str(result.get('msg') or '')[:120],
        })
        task['exec_history'] = history[-10:]


    def _compute_refill_deadline(self, refill_task):
        mode = str(refill_task.get('deadline_mode') or 'absolute').strip()
        if mode == 'before_start':
            date_str = str(refill_task.get('date') or '').strip()
            times = sorted([str(t).strip() for t in (refill_task.get('target_times') or []) if str(t).strip()])
            if not date_str or not times:
                return None, ''
            time0 = times[0]
            try:
                start_dt = datetime.strptime(f"{date_str} {time0}:00" if len(time0) == 5 else f"{date_str} {time0}", "%Y-%m-%d %H:%M:%S")
                before_h = max(0.0, float(refill_task.get('deadline_before_hours') or 0.0))
                deadline_dt = start_dt - timedelta(hours=before_h)
                return deadline_dt, deadline_dt.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                return None, ''

        deadline_raw = str(refill_task.get('deadline') or '').strip()
        if not deadline_raw:
            return None, ''
        try:
            return datetime.strptime(deadline_raw, "%Y-%m-%d %H:%M:%S"), deadline_raw
        except Exception:
            return None, deadline_raw

    def _should_notify_refill_success(self, task_id):
        bucket = datetime.now().strftime("%Y%m%d%H%M")
        key = f"{task_id}:{bucket}"
        if self._refill_notify_last_bucket.get(str(task_id)) == bucket:
            return False
        self._refill_notify_last_bucket[str(task_id)] = bucket
        return True

    def _run_refill_task_once(self, refill_task, source='auto'):
        task_id = str(refill_task.get('id') or 'unknown')
        date_str = str(refill_task.get('date') or '').strip()
        target_times = [str(t).strip() for t in (refill_task.get('target_times') or []) if str(t).strip()]
        candidate_places = [str(p).strip() for p in (refill_task.get('candidate_places') or []) if str(p).strip()]
        target_count = max(1, min(MAX_TARGET_COUNT, int(refill_task.get('target_count', 1) or 1)))
        tag = f"[refill#{task_id}|{source}]"

        deadline_dt, deadline_text = self._compute_refill_deadline(refill_task)
        if deadline_dt and datetime.now() >= deadline_dt:
            msg = f"å·²è¶…è¿‡æˆªæ­¢æ—¶é—´({deadline_text})ï¼Œåœæ­¢æ‰§è¡Œ"
            log(f"â¹ï¸ {tag} {msg}")
            return {'status': 'stopped', 'msg': msg}

        log(f"ğŸ§© {tag} å¼€å§‹æ‰§è¡Œ: date={date_str}, target_count={target_count}, times={target_times}, places={len(candidate_places)}")
        if not date_str or not target_times or not candidate_places:
            msg = 'refillä»»åŠ¡ç¼ºå°‘ date/target_times/candidate_places'
            log(f"âŒ {tag} {msg}")
            return {'status': 'fail', 'msg': msg}

        need_res = {'need_by_time': {}}
        orders_res = client.get_place_orders()
        mine_slots = set()
        if 'error' not in orders_res:
            mine_slots = client._extract_mine_slots(orders_res.get('data', []), date_str)

        for t in target_times:
            mine_cnt = sum(1 for p in candidate_places if (p, t) in mine_slots)
            need_res['need_by_time'][t] = max(0, target_count - mine_cnt)

        if sum(need_res['need_by_time'].values()) <= 0:
            msg = 'refillç›®æ ‡å·²æ»¡è¶³'
            log(f"âœ… {tag} {msg}")
            return {'status': 'success', 'msg': msg, 'success_items': []}

        matrix_res = client.get_matrix(date_str)
        if 'error' in matrix_res:
            msg = f"è·å–çŸ©é˜µå¤±è´¥: {matrix_res.get('error')}"
            log(f"âŒ {tag} {msg}")
            return {'status': 'error', 'msg': msg}
        matrix = matrix_res.get('matrix') or {}

        picks = []
        for t in target_times:
            remain = int(need_res['need_by_time'].get(t, 0))
            if remain <= 0:
                continue
            for p in candidate_places:
                if remain <= 0:
                    break
                if matrix.get(p, {}).get(t) == 'available':
                    picks.append({'place': p, 'time': t})
                    remain -= 1

        if not picks:
            msg = f"å½“å‰æ— å¯è¡¥è®¢ç»„åˆï¼Œç¼ºå£: {need_res['need_by_time']}"
            log(f"ğŸ™ˆ {tag} {msg}")
            return {'status': 'fail', 'msg': msg}

        log(f"ğŸ“¦ {tag} æœ¬è½®æäº¤: {picks}")
        submit_res = client.submit_order(date_str, picks)
        log(f"ğŸ§¾ {tag} æœ¬è½®ç»“æœ: {submit_res.get('status')} - {submit_res.get('msg')}")
        if submit_res.get('status') in ('success', 'partial') and (submit_res.get('success_items') or []):
            ok_items = submit_res.get('success_items') or []
            item_text = 'ã€'.join([f"{it.get('place')}å·{it.get('time')}" for it in ok_items[:6]])
            msg = f"Refill#{task_id}è¡¥è®¢æˆåŠŸ({len(ok_items)}é¡¹): {date_str} {item_text}"
            if self._should_notify_refill_success(task_id):
                self.send_notification(msg)
                self.send_wechat_notification(msg)
            else:
                log(f"ğŸ”• {tag} æœ¬åˆ†é’Ÿå†…å·²é€šçŸ¥ï¼Œè·³è¿‡é‡å¤æˆåŠŸé€šçŸ¥")
        return submit_res

    def run_refill_scheduler_tick(self):
        now = time.time()
        for t in list(self.refill_tasks):
            if not bool(t.get('enabled', True)):
                continue
            tid = int(t.get('id', 0))
            deadline_dt, deadline_text = self._compute_refill_deadline(t)
            if deadline_dt and datetime.now() >= deadline_dt:
                t['enabled'] = False
                t['last_result'] = {'status': 'stopped', 'msg': f'è¾¾åˆ°æˆªæ­¢æ—¶é—´({deadline_text})ï¼Œè‡ªåŠ¨åœç”¨'}
                self.append_refill_history(t, t['last_result'])
                self.save_refill_tasks()
                try:
                    task_id = str(t.get('id') or '-')
                    date_str = str(t.get('date') or '')
                    content = f"Refill#{task_id} å·²åˆ°æˆªæ­¢æ—¶é—´({deadline_text})ï¼Œä»»åŠ¡è‡ªåŠ¨åœç”¨ã€‚æ—¥æœŸ: {date_str}"
                    self.send_notification(content)
                    self.send_wechat_notification(content)
                except Exception as e:
                    log(f"âš ï¸ [refill#{t.get('id')}] æˆªæ­¢åœç”¨é€šçŸ¥å‘é€å¤±è´¥: {e}")
                continue
            interval = max(1.0, float(t.get('interval_seconds', 10.0) or 10.0))
            last = float(self._refill_last_run.get(tid, 0.0))
            if now - last < interval:
                continue
            self._refill_last_run[tid] = now
            t['last_result'] = {'status': 'running', 'msg': 'è‡ªåŠ¨è½®è¯¢æ‰§è¡Œä¸­'}
            self.save_refill_tasks()
            try:
                res = self._run_refill_task_once(t, source='auto')
            except Exception as e:
                res = {'status': 'error', 'msg': str(e)}
            t['last_run_at'] = int(time.time() * 1000)
            t['last_result'] = res
            self.append_refill_history(t, res)
            self.save_refill_tasks()

    def load_tasks(self):
        if os.path.exists(TASKS_FILE):
            try:
                with open(TASKS_FILE, 'r', encoding='utf-8') as f:
                    self.tasks = json.load(f)
            except:
                self.tasks = []
                
    def save_tasks(self):
        with open(TASKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.tasks, f, ensure_ascii=False, indent=2)
            
    def add_task(self, task):
        # task: {id, type='daily'|'weekly', run_time='08:00', target_day_offset=2, items=[...]}
        cfg = task.get('config') if isinstance(task, dict) else None
        if isinstance(cfg, dict) and 'target_count' in cfg:
            try:
                cfg['target_count'] = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))
            except Exception:
                cfg['target_count'] = 2

        task['id'] = int(time.time() * 1000)
        self.tasks.append(task)
        self.save_tasks()
        self.refresh_schedule()

    def update_task(self, task_id, task):
        task_id = int(task_id)
        for i, old in enumerate(self.tasks):
            if int(old.get('id', -1)) == task_id:
                cfg = task.get('config') if isinstance(task, dict) else None
                if isinstance(cfg, dict) and 'target_count' in cfg:
                    try:
                        cfg['target_count'] = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))
                    except Exception:
                        cfg['target_count'] = 2

                task['id'] = task_id
                task['last_run_at'] = old.get('last_run_at')
                self.tasks[i] = task
                self.save_tasks()
                self.refresh_schedule()
                return True
        return False

    def mark_task_run(self, task_id):
        task_id = int(task_id)
        for task in self.tasks:
            if int(task.get('id', -1)) == task_id:
                task['last_run_at'] = int(time.time() * 1000)
                self.save_tasks()
                return

    def delete_task(self, task_id, refresh=True):
        self.tasks = [t for t in self.tasks if t['id'] != int(task_id)]
        self.save_tasks()
        if refresh:
            self.refresh_schedule()

    def send_notification(self, content, phones=None):
        """
        å‘é€çŸ­ä¿¡é€šçŸ¥ï¼š
        - phones ä¸ä¸º None æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å·ç ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰
        - å¦åˆ™é€€å›åˆ°å…¨å±€ CONFIG['notification_phones']
        """
        if phones is None:
            phones = CONFIG.get('notification_phones', [])

        # å½’ä¸€åŒ–æ‰‹æœºå·ï¼šå…è®¸å­—ç¬¦ä¸²/åˆ—è¡¨æ··ç”¨
        if isinstance(phones, str):
            phones = [p.strip() for p in phones.split(',') if p.strip()]
        elif isinstance(phones, list):
            phones = [str(p).strip() for p in phones if str(p).strip()]

        if not phones:
            log(f"âš ï¸ æœªé…ç½®çŸ­ä¿¡æ‰‹æœºå·ï¼Œé€šçŸ¥å†…å®¹æœªå‘é€: {content}")
            return  # æ²¡æœ‰å·ç å°±ç›´æ¥è¿”å›

        log(f"ğŸ“§ æ­£åœ¨å‘é€çŸ­ä¿¡é€šçŸ¥ç»™: {phones}")
        try:
            u = CONFIG['sms']['user']
            p = CONFIG['sms']['api_key']

            error_map = {
                '0': 'å‘é€æˆåŠŸ',
                '30': 'å¯†ç é”™è¯¯',
                '40': 'è´¦å·ä¸å­˜åœ¨',
                '41': 'ä½™é¢ä¸è¶³',
                '42': 'å¸å·è¿‡æœŸ',
                '43': 'IPåœ°å€é™åˆ¶',
                '50': 'å†…å®¹å«æœ‰æ•æ„Ÿè¯',
                '51': 'æ‰‹æœºå·ç ä¸æ­£ç¡®'
            }

            m = ",".join(phones)
            c = f"ã€æ•°æ•°äº‘ç«¯ã€‘{content}"

            params = {
                "u": u,
                "p": p,
                "m": m,
                "c": c
            }

            resp = requests.get("https://api.smsbao.com/sms", params=params, timeout=10)

            code = resp.text
            msg = error_map.get(code, f"æœªçŸ¥é”™è¯¯({code})")
            log(f"ğŸ“§ çŸ­ä¿¡æ¥å£è¿”å›: [{code}] {msg}")

            if code != '0':
                log(f"âš ï¸ çŸ­ä¿¡å‘é€å¼‚å¸¸: {msg}")
                return False, msg
            return True, "å‘é€æˆåŠŸ"

        except Exception as e:
            log(f"âŒ çŸ­ä¿¡å‘é€å¼‚å¸¸: {e}")
            return False, str(e)

    def send_wechat_notification(self, content, tokens=None):
        """
        å‘é€å¾®ä¿¡é€šçŸ¥ï¼ˆPushPlusï¼‰ï¼š
        - tokens ä¸ä¸º None æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ tokenï¼ˆä»»åŠ¡çº§åˆ«ï¼‰
        - å¦åˆ™é€€å›åˆ°å…¨å±€ CONFIG['pushplus_tokens']
        """
        if tokens is None:
            tokens = CONFIG.get('pushplus_tokens', [])

        if isinstance(tokens, str):
            tokens = [t.strip() for t in tokens.split(',') if t.strip()]
        elif isinstance(tokens, list):
            tokens = [str(t).strip() for t in tokens if str(t).strip()]

        if not tokens:
            log(f"âš ï¸ æœªé…ç½® PushPlus tokenï¼Œå¾®ä¿¡é€šçŸ¥æœªå‘é€: {content}")
            return False, "æœªé…ç½® PushPlus token"

        try:
            payload = {
                "title": "åœºåœ°é¢„è®¢é€šçŸ¥",
                "content": content,
                "template": "txt",
            }
            for token in tokens:
                payload["token"] = token
                resp = requests.post(
                    "http://www.pushplus.plus/send",
                    json=payload,
                    timeout=10,
                )
                try:
                    data = resp.json()
                except ValueError:
                    data = {"code": -1, "msg": resp.text}
                if data.get("code") != 200:
                    log(f"âš ï¸ PushPlus å‘é€å¤±è´¥: {data}")
                else:
                    log("ğŸ“© PushPlus å‘é€æˆåŠŸ")
            return True, "å‘é€æˆåŠŸ"
        except Exception as e:
            log(f"âŒ PushPlus å‘é€å¼‚å¸¸: {e}")
            return False, str(e)

    def execute_task(self, task):
        log(f"â° [è‡ªåŠ¨ä»»åŠ¡] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.get('id')}")
        if task.get('id') is not None:
            self.mark_task_run(task['id'])

        run_started_ts = time.time()
        run_metrics = {
            "task_id": task.get('id'),
            "task_type": task.get('type', 'daily'),
            "started_at": int(run_started_ts * 1000),
            "active_started_at": int(run_started_ts * 1000),
            "prestart_wait_ms": 0,
            "attempt_count": 0,
            "first_matrix_ok_ms": None,
            "first_submit_ms": None,
            "submit_latencies_ms": [],
            "first_success_ms": None,
            "result_status": None,
            "result_msg": None,
            "target_date": None,
            "saw_locked": False,
            "unlocked_after_locked": False,
            "config_snapshot": {
                "retry_interval": float(CONFIG.get("retry_interval", 1.0) or 1.0),
                "aggressive_retry_interval": float(CONFIG.get("aggressive_retry_interval", 0.3) or 0.3),
                "batch_retry_times": int(CONFIG.get("batch_retry_times", 2) or 2),
                "batch_retry_interval": float(CONFIG.get("batch_retry_interval", 0.5) or 0.5),
                "submit_batch_size": int(CONFIG.get("submit_batch_size", 3) or 3),
                "initial_submit_batch_size": int(CONFIG.get("initial_submit_batch_size", CONFIG.get("submit_batch_size", 3)) or 3),
                "submit_timeout_seconds": float(CONFIG.get("submit_timeout_seconds", 4.0) or 4.0),
                "submit_split_retry_times": int(CONFIG.get("submit_split_retry_times", 1) or 1),
                "batch_min_interval": float(CONFIG.get("batch_min_interval", 0.8) or 0.8),
                "order_query_timeout_seconds": float(CONFIG.get("order_query_timeout_seconds", 2.5) or 2.5),
                "order_query_max_pages": int(CONFIG.get("order_query_max_pages", 2) or 2),
                "locked_retry_interval": float(CONFIG.get("locked_retry_interval", 1.0) or 1.0),
                "locked_max_seconds": float(CONFIG.get("locked_max_seconds", 60.0) or 60.0),
                "open_retry_seconds": float(CONFIG.get("open_retry_seconds", 30.0) or 30.0),
                "matrix_timeout_seconds": float(CONFIG.get("matrix_timeout_seconds", 3.0) or 3.0),
                "stop_on_none_stage_without_refill": bool(CONFIG.get("stop_on_none_stage_without_refill", False)),
                "preselect_enabled": bool(CONFIG.get("preselect_enabled", True)),
                "preselect_ttl_seconds": float(CONFIG.get("preselect_ttl_seconds", 2.0) or 2.0),
            },
            "goal_achieved": False,
            "success_item_count": 0,
            "failed_item_count": 0,
            "preselect_hit_count": 0,
            "preselect_miss_count": 0,
        }

        active_started_ts = run_started_ts

        # æ¯ä¸ªä»»åŠ¡è‡ªå·±é…ç½®çš„é€šçŸ¥æ‰‹æœºå·ï¼ˆåˆ—è¡¨ï¼‰ï¼Œç”¨äºâ€œä¸‹å•æˆåŠŸâ€ç±»é€šçŸ¥
        task_phones = task.get('notification_phones') or None
        task_pushplus_tokens = task.get('pushplus_tokens') or None
        task_id = task.get('id')
        last_fail_reason = None

        def build_date_display(date_str):
            try:
                dt = datetime.strptime(date_str, "%Y-%m-%d")
                weekday_map = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]
                weekday_label = weekday_map[dt.weekday()]
                return dt.strftime("%Y-%m-%d") + f"ï¼ˆ{weekday_label}ï¼‰"
            except Exception:
                return date_str

        def notify_task_result(success, message, items=None, date_str=None, partial=False):
            if partial:
                prefix = "é¢„è®¢éƒ¨åˆ†æˆåŠŸã€‚"
            else:
                prefix = "é¢„è®¢æˆåŠŸã€‚" if success else "ã€é¢„è®¢å¤±è´¥ã€‘"
            details = message
            if (success or partial) and date_str and items:
                success_pairs = []
                seen = set()
                for it in items:
                    p = it.get("place")
                    t = it.get("time")
                    if p is None or not t:
                        continue
                    key = f"{p}|{t}"
                    if key in seen:
                        continue
                    seen.add(key)
                    success_pairs.append(f"{p}å·{t}")
                pair_text = "ã€".join(success_pairs) if success_pairs else message
                details = f"{build_date_display(date_str)}ï¼Œ{pair_text}"
            elif date_str:
                details = f"{build_date_display(date_str)} {message}"
            content = f"{prefix}{details}"
            self.send_notification(content, phones=task_phones)
            self.send_wechat_notification(content, tokens=task_pushplus_tokens)

            run_metrics["result_status"] = "success" if success else ("partial" if partial else "fail")
            run_metrics["result_msg"] = str(message or "")[:200]
            if success:
                run_metrics["goal_achieved"] = True
                run_metrics["failed_item_count"] = 0
                if items:
                    run_metrics["success_item_count"] = max(int(run_metrics.get("success_item_count") or 0), len(items))
            if date_str:
                run_metrics["target_date"] = str(date_str)
            if (success or partial) and run_metrics.get("first_success_ms") is None:
                run_metrics["first_success_ms"] = int(max(0.0, time.time() - active_started_ts) * 1000)

        def finalize_run_metrics(date_str=None):
            try:
                now_ts = time.time()
                run_metrics["finished_at"] = int(now_ts * 1000)
                run_metrics["duration_ms"] = int(max(0.0, now_ts - run_started_ts) * 1000)
                run_metrics["active_duration_ms"] = int(max(0.0, now_ts - active_started_ts) * 1000)
                if date_str and not run_metrics.get("target_date"):
                    run_metrics["target_date"] = str(date_str)
                samples = sorted(int(x) for x in (run_metrics.get("submit_latencies_ms") or []) if x is not None)
                run_metrics["submit_latency_p50_ms"] = int(_percentile(samples, 0.5)) if samples else None
                run_metrics["submit_latency_p95_ms"] = int(_percentile(samples, 0.95)) if samples else None
                run_metrics["success_within_60s"] = bool(
                    run_metrics.get("first_success_ms") is not None and int(run_metrics.get("first_success_ms") or 0) <= 60000
                )
                append_task_run_metric(run_metrics)
                log(
                    f"ğŸ“Š [run-metric] task={run_metrics.get('task_id')} attempts={run_metrics.get('attempt_count')} "
                    f"first_matrix={run_metrics.get('first_matrix_ok_ms')}ms first_submit={run_metrics.get('first_submit_ms')}ms "
                    f"first_success={run_metrics.get('first_success_ms')}ms p95={run_metrics.get('submit_latency_p95_ms')}ms"
                )
            except Exception as e:
                log(f"âš ï¸ [run-metric] æ±‡æ€»å¤±è´¥: {e}")

        # 0. å…ˆæ£€æŸ¥ token æ˜¯å¦æœ‰æ•ˆï¼ˆåªè®°å½•æ—¥å¿—ï¼Œä¸ç«‹åˆ»æŠ¥è­¦ï¼‰
        #    ä»¥â€œè·å–åœºåœ°çŠ¶æ€å¼‚å¸¸â€ä¸ºå‡†è§¦å‘çŸ­ä¿¡æé†’ï¼Œé¿å…è¯¯æŠ¥
        is_valid, token_msg = client.check_token()
        if not is_valid:
            log(f"âš ï¸ Token å¯èƒ½å·²å¤±æ•ˆï¼Œä½†ç»§ç»­å°è¯•è·å–åœºåœ°çŠ¶æ€: {token_msg}")

        # 1. è®¡ç®—ç›®æ ‡æ—¥æœŸ
        # æ–°å¢ target_mode / target_date æ”¯æŒï¼š
        # - target_mode == 'fixed' ä¸”æœ‰ target_date æ—¶ï¼Œç›´æ¥ä½¿ç”¨è¯¥æ—¥æœŸ
        # - å¦åˆ™é€€å›åˆ°æ—§é€»è¾‘ï¼šä½¿ç”¨ target_day_offset å»¶å N å¤©
        target_mode = task.get('target_mode', 'offset')
        if target_mode == 'fixed' and task.get('target_date'):
            target_date = str(task['target_date'])
        else:
            offset_days = int(task.get('target_day_offset', 0))
            run_time = str(task.get('run_time') or '00:00:00')
            if len(run_time) == 5:
                run_time += ':00'
            try:
                hh, mm, ss = [int(x) for x in run_time.split(':')[:3]]
            except Exception:
                hh, mm, ss = 0, 0, 0

            aligned_now = client.get_aligned_now()
            base_run = aligned_now.replace(hour=hh, minute=mm, second=ss, microsecond=0)
            # è°ƒåº¦çº¿ç¨‹è§¦å‘å’ŒæœåŠ¡ç«¯æ—¶é—´å­˜åœ¨ç§’çº§åå·®ï¼Œç»™ä¸€ä¸ªå°å®½é™é¿å…â€œåˆšè¿‡ç‚¹å°±æ»šåˆ°æ˜å¤©/ä¸‹å‘¨â€
            trigger_grace_seconds = 90
            t_type = task.get('type', 'daily')
            if t_type in ('daily', 'once'):
                if (aligned_now - base_run).total_seconds() > trigger_grace_seconds:
                    base_run = base_run + timedelta(days=1)
            elif t_type == 'weekly':
                current_weekday = aligned_now.weekday()  # å‘¨ä¸€=0
                target_weekday = int(task.get('weekly_day', 0))
                diff = target_weekday - current_weekday
                if diff < 0:
                    diff += 7
                elif diff == 0 and (aligned_now - base_run).total_seconds() > trigger_grace_seconds:
                    diff += 7
                base_run = base_run + timedelta(days=diff)

            target_date = (base_run + timedelta(days=offset_days)).strftime("%Y-%m-%d")
            log(
                f"ğŸ•’ [æ—¶é—´å¯¹é½] server_offset={round(client.server_time_offset_seconds, 3)}s, "
                f"base_run={base_run.strftime('%Y-%m-%d %H:%M:%S')}, target_date={target_date}"
            )

            aligned_now_after = client.get_aligned_now()
            if aligned_now_after < base_run:
                wait_s = (base_run - aligned_now_after).total_seconds()
                if 0 < wait_s <= 120:
                    log(f"â³ [æ—¶é—´å¯¹é½] æœåŠ¡ç«¯æœªåˆ°è§¦å‘æ—¶åˆ»ï¼Œç­‰å¾… {round(wait_s, 2)}s åå¼€å§‹æŠ¢ç¥¨")
                    time.sleep(wait_s)

        active_started_ts = time.time()
        run_metrics["active_started_at"] = int(active_started_ts * 1000)
        run_metrics["prestart_wait_ms"] = int(max(0.0, active_started_ts - run_started_ts) * 1000)

        config = task.get('config')

        # 2. å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ config æ˜¯ dict
        if not isinstance(config, dict):
            if config is not None:
                log(f"âš ï¸ è­¦å‘Š: ä»»åŠ¡ {task.get('id')} çš„ config å­—æ®µç±»å‹å¼‚å¸¸ ({type(config)})ï¼Œå·²é‡ç½®ä¸ºç©ºå­—å…¸")
            config = {}

        # 3. æ—§ç‰ˆå…¼å®¹ï¼šæ²¡æœ‰æ–°é…ç½®æ—¶èµ°æœ€æ—©çš„ items é€»è¾‘
        if not config and 'items' in task:
            res = client.submit_order(target_date, task['items'])
            status = res.get("status")
            if status == "success":
                notify_task_result(True, "å·²é¢„è®¢", items=res.get('success_items') or task['items'], date_str=target_date)
            elif status == "partial":
                notify_task_result(False, "éƒ¨åˆ†æˆåŠŸ", items=res.get('success_items') or task['items'], date_str=target_date, partial=True)
            else:
                notify_task_result(False, f"ä¸‹å•å¤±è´¥ï¼š{res.get('msg')}", items=task['items'], date_str=target_date)
            finalize_run_metrics(target_date)
            return

        # 4. è¿™æ¬¡ä»»åŠ¡çœŸæ­£å…³å¿ƒçš„ (åœºåœ°, æ—¶é—´) ç»„åˆï¼Œç”¨æ¥åˆ¤æ–­æ˜¯å¦è¿˜åœ¨â€œé”å®šæœªå¼€æ”¾â€é˜¶æ®µ
        def enumerate_candidate_pairs(cfg):
            pairs = set()
            mode = cfg.get('mode', 'normal')
            target_times = cfg.get('target_times', [])

            if mode in ('normal', 'pipeline'):
                for p in cfg.get('candidate_places', []):
                    for t in target_times:
                        pairs.add((str(p), t))

            elif mode == 'priority':
                sequences = cfg.get('priority_sequences', [])
                for t in target_times:
                    for seq in sequences:
                        for p in seq:
                            pairs.add((str(p), t))

            elif mode == 'time_priority':
                candidate_places = [str(p) for p in cfg.get('candidate_places', [])]
                if not candidate_places:
                    candidate_places = [str(i) for i in range(1, 16)]
                sequences = cfg.get('priority_time_sequences', []) or [[t] for t in target_times]
                for seq in sequences:
                    for t in seq:
                        for p in candidate_places:
                            pairs.add((p, t))
            return pairs

        def calc_pipeline_deadline(cfg, date_str):
            pipeline_cfg = cfg.get('pipeline') if isinstance(cfg.get('pipeline'), dict) else {}
            mode = str(pipeline_cfg.get('greedy_end_mode') or '').strip()

            abs_raw = str(pipeline_cfg.get('greedy_end_time') or '').strip()
            if abs_raw:
                try:
                    return datetime.strptime(abs_raw, "%Y-%m-%d %H:%M:%S")
                except Exception:
                    pass

            if mode == 'before_start':
                hours_raw = pipeline_cfg.get('greedy_end_before_hours', 24)
                try:
                    hours = float(hours_raw)
                except Exception:
                    hours = 24.0
                times = [str(t).strip() for t in (cfg.get('target_times') or []) if str(t).strip()]
                if times:
                    start_time = sorted(times)[0]
                    fmt = "%Y-%m-%d %H:%M:%S" if len(start_time) == 8 else "%Y-%m-%d %H:%M"
                    try:
                        start_dt = datetime.strptime(f"{date_str} {start_time}", fmt)
                        return start_dt - timedelta(hours=hours)
                    except Exception:
                        return None
            return None

        def build_pipeline_cfg(cfg):
            pipe = cfg.get('pipeline') if isinstance(cfg.get('pipeline'), dict) else {}
            stages = pipe.get('stages') if isinstance(pipe.get('stages'), list) else []
            if not stages:
                stages = [
                    {"type": "continuous", "enabled": True, "window_seconds": 8},
                    {"type": "random", "enabled": True, "window_seconds": 12},
                    {"type": "refill", "enabled": False, "interval_seconds": 15},
                ]
            return {
                "stages": stages,
                "stop_when_reached": bool(pipe.get('stop_when_reached', True)),
                "continuous_prefer_adjacent": bool(pipe.get('continuous_prefer_adjacent', True)),
                "no_progress_switch_rounds": max(1, int(pipe.get('no_progress_switch_rounds', 2) or 2)),
            }

        def calc_pipeline_need(cfg, date_str):
            nonlocal pipeline_last_known_mine_slots
            target_times = [str(t) for t in (cfg.get('target_times') or [])]
            candidate_places = [str(p) for p in (cfg.get('candidate_places') or [])]
            target_count = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))

            task_scope = {(p, t) for p in candidate_places for t in target_times}
            mine_slots = set()
            orders_res = client.get_place_orders()
            if "error" not in orders_res:
                mine_slots = client._extract_mine_slots(orders_res.get("data", []), date_str)
                pipeline_last_known_mine_slots = set(mine_slots)
            else:
                if isinstance(pipeline_last_known_mine_slots, set) and pipeline_last_known_mine_slots:
                    mine_slots = set(pipeline_last_known_mine_slots)
                    log(f"âš ï¸ [pipeline] è®¢å•æ‹‰å–å¤±è´¥ï¼Œä½¿ç”¨æœ€è¿‘ä¸€æ¬¡æˆåŠŸè®¢å•å¿«ç…§: {orders_res.get('error')}")
                else:
                    log(f"âš ï¸ [pipeline] è®¢å•æ‹‰å–å¤±è´¥ï¼ŒæŒ‰0å ä½å¤„ç†: {orders_res.get('error')}")

            task_mine = mine_slots & task_scope
            need_by_time = {}
            for t in target_times:
                mine_count = sum(1 for p in candidate_places if (p, t) in task_mine)
                need_by_time[t] = max(0, target_count - mine_count)

            return {
                "task_scope": task_scope,
                "task_mine": task_mine,
                "need_by_time": need_by_time,
                "target_times": target_times,
                "candidate_places": candidate_places,
                "target_count": target_count,
            }

        def choose_pipeline_items(matrix, need_res, stage_type, prefer_adjacent=True, pair_fail_cache=None, biz_fail_cooldown_seconds=15.0):
            target_times = need_res['target_times']
            candidate_places = need_res['candidate_places']
            need_by_time = dict(need_res['need_by_time'])
            items = []
            picked_pairs = set()

            def add_pick(p, t):
                key = (str(p), str(t))
                if key in picked_pairs:
                    return False
                if int(need_by_time.get(t, 0)) <= 0:
                    return False
                if matrix.get(str(p), {}).get(str(t)) != 'available':
                    return False
                if isinstance(pair_fail_cache, dict):
                    fail_meta = pair_fail_cache.get(key)
                    if fail_meta and fail_meta.get('type') == 'biz':
                        if (time.time() - float(fail_meta.get('ts', 0.0))) < float(biz_fail_cooldown_seconds):
                            return False
                picked_pairs.add(key)
                items.append({"place": str(p), "time": str(t)})
                need_by_time[str(t)] = max(0, int(need_by_time.get(str(t), 0)) - 1)
                return True

            # continuous é˜¶æ®µå…ˆåšâ€œè·¨æ—¶æ®µäº¤é›†é€‰åœºâ€ï¼Œä¼˜å…ˆæŠŠåŒä¸€å—åœºåœ¨å¤šä¸ªæ—¶æ®µä¸€èµ·æ‹¿ä¸‹ã€‚
            if stage_type == 'continuous':
                required_times = [t for t in target_times if int(need_by_time.get(t, 0)) > 0]
                if required_times:
                    avail_all = [
                        str(p)
                        for p in candidate_places
                        if all(matrix.get(str(p), {}).get(str(t)) == 'available' for t in required_times)
                    ]
                    if avail_all:
                        if prefer_adjacent:
                            nums = sorted({int(p) for p in avail_all if str(p).isdigit()})
                            best = []
                            run = []
                            for n in nums:
                                if not run or n == run[-1] + 1:
                                    run.append(n)
                                else:
                                    if len(run) > len(best):
                                        best = run
                                    run = [n]
                            if len(run) > len(best):
                                best = run

                            if best:
                                ordered = [str(n) for n in best] + [p for p in avail_all if p not in {str(n) for n in best}]
                            else:
                                ordered = list(avail_all)
                        else:
                            ordered = list(avail_all)

                        court_need = max(int(need_by_time.get(t, 0)) for t in required_times)
                        for p in ordered[:max(0, court_need)]:
                            for t in required_times:
                                add_pick(p, t)

            # ç¬¬äºŒæ­¥ï¼šæŒ‰æ—¶æ®µè¡¥é½å‰©ä½™ç¼ºå£ï¼ˆcontinuous/random éƒ½ä¼šèµ°è¿™æ­¥ï¼‰
            for t in target_times:
                need = int(need_by_time.get(t, 0))
                if need <= 0:
                    continue
                avail = [str(p) for p in candidate_places if matrix.get(str(p), {}).get(str(t)) == 'available']
                if not avail:
                    continue

                if stage_type == 'continuous':
                    if prefer_adjacent:
                        nums = sorted({int(p) for p in avail if str(p).isdigit()})
                        best = []
                        run = []
                        for n in nums:
                            if not run or n == run[-1] + 1:
                                run.append(n)
                            else:
                                if len(run) > len(best):
                                    best = run
                                run = [n]
                        if len(run) > len(best):
                            best = run
                        ordered = [str(n) for n in best] + [p for p in avail if p not in {str(n) for n in best}] if best else avail
                    else:
                        ordered = list(avail)
                else:
                    ordered = list(avail)
                    random.shuffle(ordered)

                if stage_type != 'continuous' and isinstance(pair_fail_cache, dict):
                    def _pair_score(px):
                        meta = pair_fail_cache.get((str(px), str(t))) or {}
                        if meta.get('type') == 'network':
                            return (0, -float(meta.get('ts', 0.0)))
                        if meta.get('type') == 'biz':
                            return (2, float(meta.get('ts', 0.0)))
                        return (1, 0.0)
                    ordered = sorted(ordered, key=_pair_score)

                for p in ordered:
                    if int(need_by_time.get(t, 0)) <= 0:
                        break
                    add_pick(p, t)

            return items

        # === æ™ºèƒ½æŠ¢ç¥¨æ ¸å¿ƒé€»è¾‘ ===
        retry_interval = CONFIG.get('retry_interval', 0.5)
        aggressive_retry_interval = CONFIG.get('aggressive_retry_interval', 0.3)

        # æ–°å¢ï¼šé”å®šçŠ¶æ€ä¸‹çš„é‡è¯•é—´éš” & æœ€å¤šç­‰å¾…æ—¶é—´
        locked_retry_interval = CONFIG.get('locked_retry_interval', retry_interval)
        locked_max_seconds = CONFIG.get('locked_max_seconds', 60)
        open_retry_seconds = CONFIG.get('open_retry_seconds', 30)

        # è®°å½•è¿›å…¥ã€Œé”å®šç­‰å¾…æ¨¡å¼ã€çš„èµ·å§‹æ—¶é—´ï¼Œç”¨äºç»Ÿè®¡å·²ç­‰å¾…å¤šä¹…
        locked_mode_started_at = None
        # è®°å½•è¿›å…¥ã€Œå·²å¼€æ”¾ä½†æ— å¯ç”¨ç»“æœã€çŠ¶æ€çš„èµ·å§‹æ—¶é—´
        open_mode_started_at = None
        # pipeline çŠ¶æ€
        pipeline_started_at = None
        pipeline_refill_last_at = 0.0
        pipeline_force_random_after_continuous = False
        pipeline_no_progress_rounds = 0
        pipeline_need_before_submit = None
        pipeline_none_stage_without_refill = False
        pipeline_last_known_mine_slots = None
        pair_fail_cache = {}
        pair_fail_cache_ttl_s = 120.0
        pair_fail_cache_max = 300
        has_submitted_once = False
        preselect_cache = None

        def _preselect_candidates_from_need(matrix, need_res, prefer_adjacent=True):
            target_times = [str(t) for t in (need_res.get("target_times") or [])]
            candidate_places = [str(p) for p in (need_res.get("candidate_places") or [])]
            need_by_time = dict(need_res.get("need_by_time") or {})
            picks = []
            picked = set()

            def add_pick(p, t):
                k = (str(p), str(t))
                if k in picked:
                    return
                if int(need_by_time.get(str(t), 0)) <= 0:
                    return
                st = matrix.get(str(p), {}).get(str(t))
                if st not in ("available", "locked"):
                    return
                picks.append({"place": str(p), "time": str(t)})
                picked.add(k)
                need_by_time[str(t)] = max(0, int(need_by_time.get(str(t), 0)) - 1)

            req_times = [t for t in target_times if int(need_by_time.get(t, 0)) > 0]
            if req_times:
                common = []
                for p in candidate_places:
                    ok = True
                    score = 0
                    for t in req_times:
                        st = matrix.get(str(p), {}).get(str(t))
                        if st not in ("available", "locked"):
                            ok = False
                            break
                        score += 2 if st == "available" else 1
                    if ok:
                        common.append((str(p), score))
                if common:
                    ordered = [p for p,_ in sorted(common, key=lambda x: (-x[1], int(x[0]) if x[0].isdigit() else 999))]
                    if prefer_adjacent:
                        nums=[int(p) for p in ordered if p.isdigit()]
                        best=[];run=[]
                        for n in nums:
                            if not run or n==run[-1]+1: run.append(n)
                            else:
                                if len(run)>len(best): best=run
                                run=[n]
                        if len(run)>len(best): best=run
                        if best:
                            b=[str(n) for n in best]
                            ordered=b+[p for p in ordered if p not in set(b)]
                    court_need = max(int(need_by_time.get(t, 0)) for t in req_times)
                    for p in ordered[:max(0,court_need)]:
                        for t in req_times:
                            add_pick(p,t)

            for t in target_times:
                need = int(need_by_time.get(t, 0))
                if need <= 0:
                    continue
                avail = [str(p) for p in candidate_places if matrix.get(str(p), {}).get(str(t)) in ("available", "locked")]
                if not avail:
                    continue
                avail = sorted(avail, key=lambda p: (0 if matrix.get(str(p), {}).get(str(t))=="available" else 1, int(p) if p.isdigit() else 999))
                for p in avail:
                    if int(need_by_time.get(str(t), 0)) <= 0:
                        break
                    add_pick(p, t)
            return picks

        def compact_pair_fail_cache(now_ts=None):
            ts = float(now_ts or time.time())
            for k in list(pair_fail_cache.keys()):
                item = pair_fail_cache.get(k) or {}
                if (ts - float(item.get('ts', 0.0))) > pair_fail_cache_ttl_s:
                    pair_fail_cache.pop(k, None)
            if len(pair_fail_cache) > pair_fail_cache_max:
                extra = len(pair_fail_cache) - pair_fail_cache_max
                old_keys = sorted(pair_fail_cache.keys(), key=lambda k: float((pair_fail_cache.get(k) or {}).get('ts', 0.0)))[:extra]
                for k in old_keys:
                    pair_fail_cache.pop(k, None)

        def classify_fail_type(msg):
            text = str(msg or "").lower()
            network_keys = [
                "timeout", "timed out", "connection", "non-json", "404", "502", "503", "504",
                "nginx", "bad gateway", "service unavailable", "temporarily unavailable",
            ]
            if any(k in text for k in network_keys):
                return "network"
            return "biz"

        attempt = 0
        while True:

            # å…è®¸åœ¨è¿è¡Œè¿‡ç¨‹ä¸­é€šè¿‡ config.json è°ƒæ•´é‡è¯•é€Ÿåº¦
            retry_interval = CONFIG.get('retry_interval', retry_interval)
            aggressive_retry_interval = CONFIG.get('aggressive_retry_interval', aggressive_retry_interval)
            locked_retry_interval = CONFIG.get('locked_retry_interval', locked_retry_interval)
            locked_max_seconds = CONFIG.get('locked_max_seconds', locked_max_seconds)
            open_retry_seconds = CONFIG.get('open_retry_seconds', open_retry_seconds)

            attempt += 1
            run_metrics["attempt_count"] = int(attempt)
            compact_pair_fail_cache()
            log(f"ğŸ”„ ç¬¬ {attempt} è½®æ— é™å°è¯•...å–µ")

            # 1. è·å–æœ€æ–°åœºåœ°çŠ¶æ€
            include_mine_overlay = attempt > 1
            if not include_mine_overlay:
                log("âš¡ [åŠ é€Ÿ] é¦–è½®è·³è¿‡mineè¦†ç›–ï¼Œä¼˜å…ˆæŠ¢å å¯ç”¨åº“å­˜")
            matrix_res = client.get_matrix(target_date, include_mine_overlay=include_mine_overlay)

            # 1.1 é”™è¯¯å¤„ç†ï¼ˆæœåŠ¡å™¨å´©äº† / token å¤±æ•ˆç­‰ï¼‰
            if "error" in matrix_res:
                err_msg = matrix_res["error"]
                log(f"è·å–çŠ¶æ€å¤±è´¥: {err_msg} å–µ")

                # æœåŠ¡å™¨çŸ­æ—¶å¼‚å¸¸ï¼ˆ404/5xx/ç½‘å…³/è¶…æ—¶/éJSONç­‰ï¼‰â€”â€” æ­»ç£•æ¨¡å¼
                err_l = str(err_msg or "").lower()
                transient_keywords = [
                    "éjsonæ ¼å¼", "non-json", "404", "502", "503", "504", "æ— æ•ˆæ•°æ®",
                    "nginx", "bad gateway", "service unavailable", "timeout", "timed out",
                    "connection reset", "max retries exceeded", "temporarily unavailable",
                ]
                if any(k in err_l for k in transient_keywords):
                    log(f"âš ï¸ æ£€æµ‹åˆ°æœåŠ¡å™¨çŸ­æ—¶å¼‚å¸¸ï¼Œå¯ç”¨é«˜é¢‘é‡è¯• ({aggressive_retry_interval}s)")
                    time.sleep(aggressive_retry_interval)
                    continue

                # ä¼šè¯ / å‡­è¯å¤±æ•ˆï¼Œè¿™ç§é‡è¯•ä¹Ÿæ²¡ç”¨ï¼Œç›´æ¥æŠ¥è­¦é€€å‡º
                if "å¤±æ•ˆ" in err_msg or "å‡­è¯" in err_msg or "token" in err_msg.lower():
                    log(f"âŒ ä¸¥é‡é”™è¯¯: {err_msg}ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                    notify_task_result(False, f"ç™»å½•çŠ¶æ€/Token å¤±æ•ˆ({err_msg})ï¼Œè¯·å°½å¿«å¤„ç†ï¼", date_str=target_date)
                    finalize_run_metrics(target_date)
                    return

                # æ™®é€šé”™è¯¯ï¼šæŒ‰æ™®é€šé—´éš”é‡è¯•
                time.sleep(retry_interval)
                continue

        # æ‰§è¡Œå¾ªç¯ä¹‹å¤–ï¼šè½ç›˜æœ¬æ¬¡ä»»åŠ¡å…³é”®æŒ‡æ ‡ï¼ˆç”¨äºæ¬¡æ—¥å¤ç›˜ï¼‰
        # æ³¨æ„ï¼šæ­£å¸¸æµç¨‹åŸºæœ¬éƒ½åœ¨ while å†… returnï¼Œæœ¬æ®µä½œä¸ºå…œåº•ï¼›
        # å¦å¤–åœ¨ finally ä¸­ç»Ÿä¸€å†™ç›˜å¯è¦†ç›–ç»å¤§å¤šæ•° return è·¯å¾„ã€‚

            # 1.2 æ­£å¸¸æ‹¿åˆ°çŸ©é˜µ
            if run_metrics.get("first_matrix_ok_ms") is None:
                run_metrics["first_matrix_ok_ms"] = int(max(0.0, time.time() - active_started_ts) * 1000)
            matrix = matrix_res.get("matrix", {})

            mode_configs = config.get('modes') if isinstance(config.get('modes'), list) and config.get('modes') else [config]

            # 2. åˆ¤æ–­å½“å‰ç›®æ ‡æ˜¯å¦è¿˜æœ‰ã€Œé”å®šæœªå¼€æ”¾ã€çš„åœºæ¬¡
            locked_exists = False
            for cfg in mode_configs:
                for p, t in enumerate_candidate_pairs(cfg):
                    state = matrix.get(str(p), {}).get(t)
                    if state == "locked":
                        locked_exists = True
                        break
                if locked_exists:
                    break

            if locked_exists:
                run_metrics["saw_locked"] = True
            elif run_metrics.get("saw_locked"):
                run_metrics["unlocked_after_locked"] = True

            preselect_enabled = bool(CONFIG.get("preselect_enabled", True))
            preselect_ttl_s = max(0.2, float(CONFIG.get("preselect_ttl_seconds", 2.0) or 2.0))
            preselect_only_before_first_submit = bool(CONFIG.get("preselect_only_before_first_submit", True))

            # 3. å•ä»»åŠ¡å¤šæ¨¡å¼ï¼šæŒ‰é¡ºåºå°è¯•ï¼Œå‘½ä¸­ä¸€ä¸ªæ¨¡å¼åä»…ä½¿ç”¨è¯¥æ¨¡å¼ç»“æœï¼Œä¸è·¨æ¨¡å¼è¡¥é½
            final_items: list[dict] = []
            selected_mode = None
            selected_cfg = None
            pipeline_active_stage = None
            pipeline_cfg_for_retry = None
            pipeline_refill_wait_seconds = 0.0
            pipeline_none_stage_without_refill = False
            preselect_used = False
            if preselect_enabled and preselect_cache and not locked_exists and (not preselect_only_before_first_submit or not has_submitted_once):
                age_s = max(0.0, time.time() - float(preselect_cache.get("ts", 0.0)))
                if age_s <= preselect_ttl_s and (preselect_cache.get("date") == target_date):
                    final_items = [dict(x) for x in (preselect_cache.get("items") or [])]
                    selected_mode = preselect_cache.get("mode")
                    selected_cfg = mode_configs[int(preselect_cache.get("cfg_idx", 0))] if mode_configs else None
                    preselect_used = bool(final_items)
                    if preselect_used:
                        run_metrics["preselect_hit_count"] = int(run_metrics.get("preselect_hit_count") or 0) + 1
                        log(f"ğŸš€ [preselect] å‘½ä¸­é¢„é€‰ç»„åˆï¼Œage={round(age_s*1000)}msï¼Œç›´æ¥ä¸‹å•: {final_items}")
                else:
                    run_metrics["preselect_miss_count"] = int(run_metrics.get("preselect_miss_count") or 0) + 1

            if not preselect_used:
                for cfg_idx, cfg in enumerate(mode_configs):
                    mode = cfg.get('mode', 'normal')
                    target_times = cfg.get('target_times', [])
                    mode_items: list[dict] = []

                    # --- æ¨¡å¼ P: pipeline(continuous/random/refill) ---
                    if mode == 'pipeline':
                        pipeline_cfg_for_retry = cfg
                        now_ts = time.time()
                        if pipeline_started_at is None:
                            pipeline_started_at = now_ts

                        need_res = calc_pipeline_need(cfg, target_date)
                        pipe_cfg = build_pipeline_cfg(cfg)
                        current_need_total = sum(int(v) for v in (need_res.get('need_by_time') or {}).values())

                        if sum(need_res['need_by_time'].values()) == 0 and pipe_cfg['stop_when_reached']:
                            achieved_count = len(need_res.get("task_mine") or [])
                            run_metrics["success_item_count"] = max(int(run_metrics.get("success_item_count") or 0), achieved_count)
                            run_metrics["failed_item_count"] = 0
                            notify_task_result(True, "å·²è¾¾ä»»åŠ¡ç›®æ ‡ï¼Œæ— éœ€è¡¥é½", date_str=target_date)
                            finalize_run_metrics(target_date)
                            return

                        deadline = calc_pipeline_deadline(cfg, target_date)
                        if deadline and client.get_aligned_now() >= deadline:
                            notify_task_result(False, f"è¾¾åˆ°æˆªæ­¢æ—¶é—´({deadline.strftime('%Y-%m-%d %H:%M:%S')})ï¼Œåœæ­¢è¡¥é½", date_str=target_date)
                            finalize_run_metrics(target_date)
                            return

                        stages = pipe_cfg['stages']

                        elapsed = now_ts - pipeline_started_at
                        active_stage = None
                        consumed = 0.0
                        refill_stage = None
                        for st in stages:
                            if not isinstance(st, dict) or not st.get('enabled', True):
                                continue
                            stype = str(st.get('type') or '').strip()
                            if stype == 'refill':
                                refill_stage = st
                                continue
                            win = float(st.get('window_seconds', 0) or 0)
                            if win <= 0:
                                continue
                            if elapsed < consumed + win:
                                active_stage = st
                                break
                            consumed += win

                        if active_stage is None and refill_stage is not None:
                            active_stage = refill_stage

                        stype = str((active_stage or {}).get('type') or '').strip()
                        if stype == 'continuous' and pipeline_no_progress_rounds >= int(pipe_cfg.get('no_progress_switch_rounds', 2)):
                            log(f"ğŸ§ª [pipeline] è¿ç»­{pipeline_no_progress_rounds}è½®ç¼ºå£æœªæ”¹å–„ï¼Œæå‰åˆ‡æ¢åˆ°random")
                            stype = 'random'
                        if stype == 'continuous' and pipeline_force_random_after_continuous:
                            log("ğŸ§ª [pipeline] æ£€æµ‹åˆ°continuousé˜¶æ®µå·²å‡ºç°ç¼ºå£ï¼Œæå‰åˆ‡æ¢åˆ°randomè¡¥é½")
                            stype = 'random'
                        pipeline_active_stage = stype
                        log(f"ğŸ§ª [pipeline] å½“å‰é˜¶æ®µ={stype or 'none'} elapsed={round(elapsed, 2)}s")
                        if not stype and refill_stage is None and bool(CONFIG.get('stop_on_none_stage_without_refill', False)):
                            pipeline_none_stage_without_refill = True
                            log("ğŸ§ª [pipeline] é˜¶æ®µçª—å£å·²ç»“æŸä¸”æœªå¯ç”¨refillï¼ŒæŒ‰é…ç½®ç«‹å³ç»“æŸä»»åŠ¡")
                        if stype == 'continuous':
                            mode_items = choose_pipeline_items(matrix, need_res, 'continuous', prefer_adjacent=pipe_cfg.get('continuous_prefer_adjacent', True), pair_fail_cache=pair_fail_cache, biz_fail_cooldown_seconds=CONFIG.get('biz_fail_cooldown_seconds', 15.0))
                        elif stype == 'random':
                            mode_items = choose_pipeline_items(matrix, need_res, 'random', prefer_adjacent=pipe_cfg.get('continuous_prefer_adjacent', True), pair_fail_cache=pair_fail_cache, biz_fail_cooldown_seconds=CONFIG.get('biz_fail_cooldown_seconds', 15.0))
                        elif stype == 'refill':
                            interval = float((active_stage or {}).get('interval_seconds', 15) or 15)
                            refill_interval = max(1.0, interval)
                            refill_elapsed = now_ts - pipeline_refill_last_at
                            if refill_elapsed >= refill_interval:
                                mode_items = choose_pipeline_items(matrix, need_res, 'random', prefer_adjacent=pipe_cfg.get('continuous_prefer_adjacent', True), pair_fail_cache=pair_fail_cache, biz_fail_cooldown_seconds=CONFIG.get('biz_fail_cooldown_seconds', 15.0))
                                pipeline_refill_last_at = now_ts
                                pipeline_refill_wait_seconds = 0.0
                            else:
                                pipeline_refill_wait_seconds = max(0.0, refill_interval - refill_elapsed)
                                log(f"ğŸ§ª [pipeline-refill] æœªåˆ°ä¸‹æ¬¡è¡¥é½çª—å£ï¼Œå‰©ä½™ {round(pipeline_refill_wait_seconds, 2)}s")
                                mode_items = []
                        else:
                            mode_items = []

                        if stype in ('continuous', 'random', 'refill'):
                            pipeline_need_before_submit = current_need_total
                            if preselect_enabled and (not preselect_only_before_first_submit or not has_submitted_once):
                                pre_items = _preselect_candidates_from_need(matrix, need_res, prefer_adjacent=pipe_cfg.get('continuous_prefer_adjacent', True))
                                if pre_items:
                                    preselect_cache = {"items": pre_items, "ts": time.time(), "date": target_date, "mode": "pipeline", "cfg_idx": cfg_idx}
                                    if locked_exists:
                                        log(f"ğŸ§  [preselect] é”å®šæœŸé¢„é€‰ç»„åˆå·²æ›´æ–°: {pre_items}")

                    # --- æ¨¡å¼ A: åœºåœ°ä¼˜å…ˆä¼˜å…ˆçº§åºåˆ— (priority) ---
                    elif mode == 'priority':
                        sequences = cfg.get('priority_sequences', [])
                        target_count = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))
                        allow_partial = cfg.get('allow_partial', True)

                        for time_slot in target_times:
                            if len(mode_items) >= target_count:
                                break
                            for seq in sequences:
                                if len(mode_items) >= target_count:
                                    break
                                if len(seq) > (target_count - len(mode_items)):
                                    continue

                                all_avail = True
                                for p in seq:
                                    if p not in matrix or matrix[p].get(time_slot) != "available":
                                        all_avail = False
                                        break

                                if all_avail:
                                    for p in seq:
                                        for item in mode_items:
                                            if item['place'] == str(p) and item['time'] == time_slot:
                                                all_avail = False
                                                break

                                if all_avail:
                                    log(f"   -> ğŸ¯ [ä¼˜å…ˆçº§-æ•´] å‘½ä¸­å®Œæ•´ç»„åˆ: {seq} @ {time_slot}")
                                    for p in seq:
                                        mode_items.append({"place": str(p), "time": time_slot})

                        if allow_partial and len(mode_items) < target_count:
                            log(f"   -> âš ï¸ [ä¼˜å…ˆçº§-æ•£] å®Œæ•´ç»„åˆä¸è¶³ï¼Œå¼€å§‹æ•£å•å¡«å…… (ç›®æ ‡{target_count}, å·²æœ‰{len(mode_items)})")
                            for time_slot in target_times:
                                if len(mode_items) >= target_count:
                                    break
                                for seq in sequences:
                                    if len(mode_items) >= target_count:
                                        break
                                    for p in seq:
                                        if p in matrix and matrix[p].get(time_slot) == "available":
                                            is_picked = False
                                            for item in mode_items:
                                                if item['place'] == str(p) and item['time'] == time_slot:
                                                    is_picked = True
                                                    break
                                            if not is_picked:
                                                log(f"   -> ğŸ§© [ä¼˜å…ˆçº§-æ•£] æ¡æ¼: {p}å· @ {time_slot}")
                                                mode_items.append({"place": str(p), "time": time_slot})
                                                if len(mode_items) >= target_count:
                                                    break

                    # --- æ¨¡å¼ B: æ—¶é—´ä¼˜å…ˆ (time_priority) ---
                    elif mode == 'time_priority':
                        sequences = cfg.get('priority_time_sequences', []) or [[t] for t in target_times]
                        candidate_places = [str(p) for p in cfg.get('candidate_places', [])]
                        if not candidate_places:
                            candidate_places = [str(i) for i in range(1, 16)]

                        target_count = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))
                        allow_partial = cfg.get('allow_partial', True)

                        for seq in sequences:
                            if len(mode_items) >= target_count:
                                break
                            for p in candidate_places:
                                if len(mode_items) >= target_count:
                                    break

                                ok = True
                                for t in seq:
                                    if p not in matrix or matrix[p].get(t) != "available":
                                        ok = False
                                        break
                                if not ok:
                                    continue

                                already = False
                                for t in seq:
                                    for item in mode_items:
                                        if item["place"] == p and item["time"] == t:
                                            already = True
                                            break
                                    if already:
                                        break
                                if already:
                                    continue

                                log(f"   -> ğŸ¯ [æ—¶é—´ä¼˜å…ˆ-æ•´] {p}å· å‘½ä¸­æ—¶é—´æ®µ {seq}")
                                for t in seq:
                                    mode_items.append({"place": p, "time": t})
                                if len(mode_items) >= target_count:
                                    break

                        if allow_partial and len(mode_items) < target_count:
                            for t in target_times:
                                if len(mode_items) >= target_count:
                                    break
                                for p in candidate_places:
                                    if len(mode_items) >= target_count:
                                        break
                                    if p in matrix and matrix[p].get(t) == "available":
                                        already = False
                                        for item in mode_items:
                                            if item["place"] == p and item["time"] == t:
                                                already = True
                                                break
                                        if not already:
                                            mode_items.append({"place": p, "time": t})
                                            log(f"   -> ğŸ§© [æ—¶é—´ä¼˜å…ˆ-æ•£] æ¡æ¼: {p}å· @ {t}")

                    # --- æ¨¡å¼ C: æ™®é€š / æ™ºèƒ½è¿å· (normal) ---
                    else:
                        if 'candidate_places' not in cfg:
                            log(f"âŒ ä»»åŠ¡é…ç½®é”™è¯¯: éä¼˜å…ˆçº§æ¨¡å¼å¿…é¡»åŒ…å« candidate_places")
                            notify_task_result(False, "ä»»åŠ¡é…ç½®é”™è¯¯ï¼šç¼ºå°‘ candidate_placesã€‚", date_str=target_date)
                            finalize_run_metrics(target_date)
                            return

                        candidate_places = [str(p) for p in cfg['candidate_places']]
                        target_courts = max(1, min(MAX_TARGET_COUNT, int(cfg.get('target_count', 2))))
                        smart_mode = cfg.get('smart_continuous', False)

                        if target_courts <= 0:
                            log("âš ï¸ ç›®æ ‡åœºåœ°æ•°é‡ target_count <= 0ï¼Œè·³è¿‡æœ¬è½®ã€‚")
                        else:
                            available_courts: list[int] = []
                            for p in candidate_places:
                                p_str = str(p)
                                ok = True
                                for t in target_times:
                                    if p_str not in matrix or matrix[p_str].get(t) != "available":
                                        ok = False
                                        break
                                if ok:
                                    available_courts.append(int(p))

                            if not available_courts:
                                log("âš ï¸ å½“å‰æ²¡æœ‰åŒæ—¶æ»¡è¶³æ‰€æœ‰æ—¶é—´æ®µçš„å€™é€‰åœºåœ°ã€‚")
                            else:
                                available_courts.sort()
                                need = min(target_courts, len(available_courts))
                                selected_courts: list[int] = []

                                if smart_mode and len(available_courts) > 1:
                                    best_run: list[int] | None = None
                                    best_len = 0
                                    i = 0
                                    while i < len(available_courts):
                                        j = i
                                        while j + 1 < len(available_courts) and                                             available_courts[j + 1] == available_courts[j] + 1:
                                            j += 1
                                        run = available_courts[i: j + 1]
                                        if len(run) > best_len:
                                            best_len = len(run)
                                            best_run = run
                                        i = j + 1

                                    if best_run:
                                        selected_courts = best_run[:need]

                                if not selected_courts:
                                    selected_courts = available_courts[:need]

                                for p_int in selected_courts:
                                    p_str = str(p_int)
                                    for t in target_times:
                                        mode_items.append({"place": p_str, "time": t})

                    if mode_items and preselect_enabled and (not preselect_only_before_first_submit or not has_submitted_once):
                        preselect_cache = {"items": [dict(x) for x in mode_items], "ts": time.time(), "date": target_date, "mode": mode, "cfg_idx": cfg_idx}
                    if mode_items:
                        final_items = mode_items
                        selected_mode = mode
                        selected_cfg = cfg
                        break

                if selected_mode and len(mode_configs) > 1:
                    log(f"ğŸ›ï¸ å•ä»»åŠ¡å¤šæ¨¡å¼å‘½ä¸­: å½“å‰ä½¿ç”¨ {selected_mode} æ¨¡å¼æäº¤ï¼Œä¸è·¨æ¨¡å¼è¡¥é½")

                if not final_items and pipeline_none_stage_without_refill:
                    notify_task_result(False, "pipelineé˜¶æ®µçª—å£å·²ç»“æŸä¸”æœªå¯ç”¨refillï¼Œåœæ­¢ç»§ç»­è½®è¯¢", date_str=target_date)
                    finalize_run_metrics(target_date)
                    return

                # 4. æäº¤è®¢å•
            if final_items:
                submit_started_at = time.time()
                if run_metrics.get("first_submit_ms") is None:
                    run_metrics["first_submit_ms"] = int(max(0.0, submit_started_at - active_started_ts) * 1000)
                log(f"æ­£åœ¨æäº¤åˆ†æ‰¹è®¢å•: {final_items}")
                res = client.submit_order(target_date, final_items)
                has_submitted_once = True
                submit_spent_s = max(0.0, time.time() - submit_started_at)
                run_metrics.setdefault("submit_latencies_ms", []).append(int(submit_spent_s * 1000))
                if selected_mode == 'pipeline' and pipeline_started_at is not None and submit_spent_s > 0:
                    # æäº¤/æ ¡éªŒè€—æ—¶ä¸åº”åæ‰ pipeline é˜¶æ®µçª—å£ï¼Œå¦åˆ™ä¼šå¯¼è‡´ random/refill é˜¶æ®µè¢«æå‰è·³è¿‡
                    pipeline_started_at += submit_spent_s
                    log(f"â±ï¸ [pipeline] æ‰£é™¤æœ¬è½®æäº¤æµæ°´è€—æ—¶ {round(submit_spent_s, 2)}sï¼Œé¿å…é˜¶æ®µçª—å£è¢«ç½‘ç»œè€—æ—¶åƒæ‰")
                log(f"[submit_orderè°ƒè¯•] æ‰¹æ¬¡å“åº”: {res}")

                status = res.get("status")

                # pipeline æ¨¡å¼ä¸‹ï¼Œå•æ¬¡æäº¤ success/partial ä¸ä»£è¡¨ä»»åŠ¡ç›®æ ‡å·²è¾¾æˆï¼›
                # è‹¥ä»æœ‰ç¼ºå£ï¼Œåº”ç»§ç»­è¿›å…¥ä¸‹ä¸€è½®ï¼ˆå« refillï¼‰è¡¥é½ã€‚
                if selected_mode == 'pipeline' and isinstance(selected_cfg, dict):
                    post_need = calc_pipeline_need(selected_cfg, target_date)
                    remaining_slots = sum(int(v) for v in (post_need.get('need_by_time') or {}).values())
                    if remaining_slots > 0:
                        if pipeline_active_stage == 'continuous' and status in ('success', 'partial'):
                            pipeline_force_random_after_continuous = True
                            log("âš¡ [pipeline] continuousé˜¶æ®µå·²æäº¤ä½†ä»æœ‰ç¼ºå£ï¼Œä¸‹ä¸€è½®å°†ç›´æ¥åˆ‡åˆ°random")
                        deadline = calc_pipeline_deadline(selected_cfg, target_date)
                        if deadline and client.get_aligned_now() >= deadline:
                            notify_task_result(False, f"è¾¾åˆ°æˆªæ­¢æ—¶é—´({deadline.strftime('%Y-%m-%d %H:%M:%S')})ï¼Œåœæ­¢è¡¥é½", date_str=target_date)
                            finalize_run_metrics(target_date)
                            return
                        need_detail = post_need.get('need_by_time') or {}
                        before_need = int(pipeline_need_before_submit if pipeline_need_before_submit is not None else remaining_slots)
                        if remaining_slots < before_need:
                            pipeline_no_progress_rounds = 0
                        else:
                            pipeline_no_progress_rounds += 1
                        log(f"ğŸ” [pipeline] æœ¬è½®æäº¤åä»ç¼º {remaining_slots} ä¸ªæ—¶æ®µï¼Œç¼ºå£æ˜ç»†: {need_detail}ï¼Œç»§ç»­è¡¥é½ä¸‹ä¸€è½®")

                        if status in ('success', 'partial'):
                            try:
                                progress_items = res.get('success_items') or final_items
                                progress_msg = f"æœ¬è½®å·²é¢„è®¢ {len(progress_items)} ä¸ªæ—¶æ®µï¼Œç¼ºå£ {remaining_slots}ï¼Œç»§ç»­è¡¥é½ä¸­"
                                notify_task_result(
                                    False,
                                    progress_msg,
                                    items=progress_items,
                                    date_str=target_date,
                                    partial=True,
                                )
                            except Exception as e:
                                log(f"âš ï¸ [pipeline] é˜¶æ®µé€šçŸ¥æ„å»ºå¤±è´¥: {e}")

                        time.sleep(retry_interval)
                        continue

                if status == "success":
                    run_metrics["success_item_count"] = max(int(run_metrics.get("success_item_count") or 0), len(res.get("success_items") or final_items or []))
                    run_metrics["failed_item_count"] = len(res.get("failed_items") or [])
                    run_metrics["goal_achieved"] = True
                    log(f"âœ… ä¸‹å•å®Œæˆ: å…¨éƒ¨æˆåŠŸ ({status})")
                    for it in (res.get('success_items') or final_items or []):
                        pair_fail_cache.pop((str(it.get('place')), str(it.get('time'))), None)
                    try:
                        notify_task_result(
                            True,
                            "å·²é¢„è®¢",
                            items=res.get('success_items') or final_items,
                            date_str=target_date,
                        )
                    except Exception as e:
                        log(f"æ„å»ºçŸ­ä¿¡å†…å®¹å¤±è´¥: {e}")
                    finalize_run_metrics(target_date)
                    return
                elif status == "partial":
                    run_metrics["success_item_count"] = max(int(run_metrics.get("success_item_count") or 0), len(res.get("success_items") or []))
                    run_metrics["failed_item_count"] = max(int(run_metrics.get("failed_item_count") or 0), len(res.get("failed_items") or []))
                    log(f"âš ï¸ ä¸‹å•å®Œæˆ: éƒ¨åˆ†æˆåŠŸ ({status})")
                    for it in (res.get('success_items') or []):
                        pair_fail_cache.pop((str(it.get('place')), str(it.get('time'))), None)
                    fail_type = classify_fail_type(res.get('msg'))
                    for it in (res.get('failed_items') or []):
                        pair_fail_cache[(str(it.get('place')), str(it.get('time')))] = {'type': fail_type, 'ts': time.time()}
                    try:
                        notify_task_result(
                            False,
                            "éƒ¨åˆ†æˆåŠŸ",
                            items=res.get('success_items') or final_items,
                            date_str=target_date,
                            partial=True,
                        )
                    except Exception as e:
                        log(f"æ„å»ºçŸ­ä¿¡å†…å®¹å¤±è´¥: {e}")
                    finalize_run_metrics(target_date)
                    return
                elif status == "verify_pending":
                    fast_retry_s = max(0.05, float(CONFIG.get("post_submit_verify_pending_retry_seconds", 0.35) or 0.35))
                    log(f"â³ æäº¤æˆåŠŸä½†éªŒè¯æœªæ”¶æ•›ï¼Œ{round(fast_retry_s, 2)}s åå¿«é€Ÿå¤æ ¸: {res.get('msg')}")
                    time.sleep(fast_retry_s)
                    continue
                else:
                    run_metrics["failed_item_count"] = max(int(run_metrics.get("failed_item_count") or 0), len(res.get("failed_items") or final_items or []))
                    log(f"âŒ ä¸‹å•å¤±è´¥: {res.get('msg')}")
                    last_fail_reason = str(res.get('msg') or "ä¸‹å•å¤±è´¥")
                    fail_type = classify_fail_type(last_fail_reason)
                    for it in (res.get('failed_items') or final_items or []):
                        k = (str(it.get('place')), str(it.get('time')))
                        pair_fail_cache[k] = {'type': fail_type, 'ts': time.time()}
                    last_fail_lower = last_fail_reason.lower()
                    if "<html" in last_fail_lower and "404" in last_fail_lower:
                        last_fail_reason = "ä¸‹å•æ¥å£æš‚æ—¶ä¸å¯ç”¨(404)"
                    elif len(last_fail_reason) > 120:
                        last_fail_reason = last_fail_reason[:120] + "..."

            # 5. æ ¹æ® locked çŠ¶æ€å†³å®šæ˜¯å¦ç»§ç»­æ­»ç£•ï¼ˆä½¿ç”¨é”å®šé…ç½® + æœ€å¤šåˆ· N ç§’ä¿æŠ¤ï¼‰
            if locked_exists:
                now_ts = time.time()
                open_mode_started_at = None

                # ç¬¬ä¸€æ¬¡å‘ç° lockedï¼Œå¼€å§‹è®¡æ—¶
                if locked_mode_started_at is None:
                    locked_mode_started_at = now_ts

                elapsed = now_ts - locked_mode_started_at

                # è¶…è¿‡é…ç½®çš„æœ€å¤§ç­‰å¾…æ—¶é—´ -> æ”¾å¼ƒæœ¬æ¬¡ä»»åŠ¡
                if elapsed >= locked_max_seconds:
                    log(
                        f"â³ å·²è¿ç»­ç­‰å¾…ã€é”å®šæœªå¼€æ”¾ã€çŠ¶æ€çº¦ {int(elapsed)} ç§’ï¼Œ"
                        f"è¾¾åˆ°ä¸Šé™ {locked_max_seconds}sï¼Œæœ¬æ¬¡ä»»åŠ¡ç»“æŸã€‚"
                    )
                    fail_msg = "é”å®šæœªå¼€æ”¾ç­‰å¾…è¶…æ—¶ï¼Œä»»åŠ¡ç»“æŸã€‚"
                    if last_fail_reason:
                        fail_msg = f"{fail_msg} å¤±è´¥åŸå› ï¼š{last_fail_reason}"
                    notify_task_result(False, fail_msg, date_str=target_date)
                    finalize_run_metrics(target_date)
                    return

                # ä»åœ¨å…è®¸èŒƒå›´å†…ï¼ŒæŒ‰é”å®šé—´éš”ç»§ç»­è½®è¯¢
                log(
                    f"â³ å½“å‰ç›®æ ‡åœºåœ°å¤„äºã€é”å®šæœªå¼€æ”¾ã€çŠ¶æ€ï¼Œç»§ç»­ç­‰å¾…ä¸‹ä¸€è½®..."
                    f" (å·²ç­‰å¾… {int(elapsed)} ç§’ / ä¸Šé™ {locked_max_seconds}s)"
                )
                time.sleep(locked_retry_interval)
                continue
            else:
                # å·²å¼€æ”¾ï¼šçŸ­çª—å£å†…ç»§ç»­é‡è¯•ï¼Œç»™â€œé‡Šæ”¾/å›æµåº“å­˜â€ç•™æœºä¼š
                locked_mode_started_at = None
                now_ts = time.time()
                if open_mode_started_at is None:
                    open_mode_started_at = now_ts
                elapsed = now_ts - open_mode_started_at

                # pipeline è¿›å…¥ refill åï¼Œä¸å— open_retry_seconds æå‰æˆªæ–­ï¼›
                # ä»¥ pipeline æˆªæ­¢æ—¶é—´ä¸ºå‡†ç»§ç»­è¡¥é½ã€‚
                if pipeline_cfg_for_retry is not None and pipeline_active_stage == 'refill':
                    deadline = calc_pipeline_deadline(pipeline_cfg_for_retry, target_date)
                    if deadline and client.get_aligned_now() >= deadline:
                        notify_task_result(False, f"è¾¾åˆ°æˆªæ­¢æ—¶é—´({deadline.strftime('%Y-%m-%d %H:%M:%S')})ï¼Œåœæ­¢è¡¥é½", date_str=target_date)
                        finalize_run_metrics(target_date)
                        return
                    refill_sleep_s = retry_interval
                    if not final_items:
                        refill_sleep_s = max(float(retry_interval), float(pipeline_refill_wait_seconds or 0.0))
                    log(
                        f"ğŸ™ˆ [pipeline-refill] å½“å‰æ— å¯ç”¨ç»„åˆï¼Œç»§ç»­è½®è¯¢è¡¥é½..."
                        f" (å·²ç­‰å¾… {int(elapsed)} ç§’ï¼›ä»¥æˆªæ­¢æ—¶é—´æ§åˆ¶ç»“æŸï¼›ä¸‹æ¬¡çº¦ {round(refill_sleep_s, 2)}s)"
                    )
                    time.sleep(refill_sleep_s)
                    continue

                if elapsed < max(0.0, float(open_retry_seconds)):
                    if final_items:
                        log(
                            f"ğŸ™ˆ åœºåœ°å·²å¼€æ”¾ä½†æœ¬è½®æäº¤æœªæˆåŠŸï¼Œç»§ç»­é‡è¯•..."
                            f" (å·²é‡è¯• {int(elapsed)} ç§’ / ä¸Šé™ {open_retry_seconds}s)"
                        )
                    else:
                        log(
                            f"ğŸ™ˆ åœºåœ°å·²å¼€æ”¾ä½†å½“å‰æ— å¯ç”¨ç»„åˆï¼Œç»§ç»­è½®è¯¢..."
                            f" (å·²ç­‰å¾… {int(elapsed)} ç§’ / ä¸Šé™ {open_retry_seconds}s)"
                        )
                    time.sleep(retry_interval)
                    continue

                log("ğŸ™ˆ ç›®æ ‡åœºåœ°å·²ç»å¼€æ”¾ä½†åœ¨é‡è¯•çª—å£å†…ä»æ— å¯ç”¨ç»„åˆï¼Œæœ¬æ¬¡ä»»åŠ¡ç»“æŸã€‚")
                fail_msg = "ç›®æ ‡åœºåœ°å·²å¼€æ”¾ä½†æ— å¯ç”¨ç»„åˆï¼Œå¯èƒ½å·²è¢«æŠ¢å®Œã€‚"
                if last_fail_reason:
                    fail_msg = f"{fail_msg} å¤±è´¥åŸå› ï¼š{last_fail_reason}"
                notify_task_result(False, fail_msg, date_str=target_date)
                finalize_run_metrics(target_date)
                return

        # print(" æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œæ”¾å¼ƒã€‚")

    def refresh_schedule(self):
        schedule.clear("task")
        print(f"ğŸ”„ [è°ƒåº¦å™¨] æ­£åœ¨åˆ·æ–°ä»»åŠ¡åˆ—è¡¨ (å…± {len(self.tasks)} ä¸ª)...")

        # å†…éƒ¨å·¥å…·å‡½æ•°ï¼šæ”¯æŒå•æ¬¡ä»»åŠ¡æ‰§è¡Œå®Œåè‡ªåŠ¨åˆ é™¤è‡ªèº«
        def make_job(t, is_once=False):
            def _job():
                print(f"â° [è°ƒåº¦å™¨] è§¦å‘ä»»åŠ¡ ID: {t['id']}")
                self.execute_task_with_lock(t)
                if is_once:
                    print(f"âœ… å•æ¬¡ä»»åŠ¡ {t['id']} æ‰§è¡Œå®Œæˆï¼Œè‡ªåŠ¨ä»ä»»åŠ¡åˆ—è¡¨ä¸­åˆ é™¤")
                    # ä¸å† refresh_scheduleï¼Œé¿å…åœ¨è°ƒåº¦å¾ªç¯é‡Œé¢‘ç¹æ¸…ç©ºé‡å»º
                    self.delete_task(t['id'], refresh=False)
                    # å‘Šè¯‰ schedule å–æ¶ˆå½“å‰ job
                    return schedule.CancelJob

            return _job

        for task in self.tasks:
            run_time = task['run_time']
            # ç¡®ä¿æ—¶é—´æ ¼å¼æ˜¯ HH:mm:ss (æœ‰çš„æµè§ˆå™¨å¯èƒ½åªè¿”å› HH:mm)
            if len(run_time) == 5:
                run_time += ":00"

            t_type = task.get('type', 'daily')

            try:
                if t_type == 'daily':
                    schedule.every().day.at(run_time).do(make_job(task, is_once=False)).tag("task")
                    print(f"   -> å·²æ·»åŠ æ¯æ—¥ä»»åŠ¡: {run_time}")
                elif t_type == 'weekly':
                    days = [
                        schedule.every().monday,
                        schedule.every().tuesday,
                        schedule.every().wednesday,
                        schedule.every().thursday,
                        schedule.every().friday,
                        schedule.every().saturday,
                        schedule.every().sunday,
                    ]
                    wd = int(task['weekly_day'])
                    days[wd].at(run_time).do(make_job(task, is_once=False)).tag("task")
                    print(f"   -> å·²æ·»åŠ æ¯å‘¨ä»»åŠ¡: å‘¨{['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥'][wd]} {run_time}")
                elif t_type == 'once':
                    # å•æ¬¡ä»»åŠ¡ï¼šåˆ°ç‚¹æ‰§è¡Œä¸€æ¬¡ï¼Œç„¶åè‡ªåŠ¨ä»ä»»åŠ¡åˆ—è¡¨å’Œè°ƒåº¦å™¨ä¸­ç§»é™¤
                    schedule.every().day.at(run_time).do(make_job(task, is_once=True)).tag("task")
                    print(f"   -> å·²æ·»åŠ å•æ¬¡ä»»åŠ¡: {run_time}ï¼ˆæ‰§è¡Œä¸€æ¬¡åè‡ªåŠ¨åˆ é™¤ï¼‰")
            except Exception as e:
                print(f"âŒ æ·»åŠ ä»»åŠ¡å¤±è´¥: {e}")






def _template_context_lines(text: str, lineno: int, radius: int = 2) -> str:
    lines = text.splitlines()
    start = max(1, lineno - radius)
    end = min(len(lines), lineno + radius)
    out = []
    for i in range(start, end + 1):
        pointer = '>>' if i == lineno else '  '
        out.append(f"{pointer} {i}: {lines[i-1]}")
    return "\n".join(out)



def auto_fix_known_template_endif_issue(template_file: str):
    """è‡ªåŠ¨ä¿®å¤å†å²ä¸Šåå¤å‡ºç°çš„é‡å¤ endif é—®é¢˜ï¼ˆæœ€å°ã€å®šå‘ä¿®å¤ï¼‰ã€‚"""
    try:
        with open(template_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        return

    fixed = re.sub(
        r"(\n\s*\{%\s*endif\s*%\}\s*\n)\s*\{%\s*endif\s*%\}(\s*\n\s*<!--\s*Tab\s*3)",
        r"\1\2",
        content,
        count=1,
    )
    if fixed != content:
        with open(template_file, 'w', encoding='utf-8') as f:
            f.write(fixed)
        print('ğŸ› ï¸ å·²è‡ªåŠ¨ä¿®å¤æ¨¡æ¿ä¸­çš„é‡å¤ endifï¼ˆTab 2/Tab 3 äº¤ç•Œå¤„ï¼‰')

def validate_templates_on_startup():
    """å¯åŠ¨å‰å¿«é€Ÿæ£€æŸ¥å…³é”®æ¨¡æ¿è¯­æ³•ï¼Œé¿å…çº¿ä¸Šè¿è¡Œæ—¶æ‰æš´éœ² TemplateSyntaxErrorã€‚"""
    template_file = os.path.join(BASE_DIR, 'templates', 'index.html')
    auto_fix_known_template_endif_issue(template_file)
    try:
        with open(template_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        raise RuntimeError(f'æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_file}')

    digest = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
    print(f'ğŸ” æ¨¡æ¿æ–‡ä»¶æ ¡éªŒ: {template_file} (md5:{digest})')

    try:
        Environment().parse(content)
        print('âœ… æ¨¡æ¿è¯­æ³•æ£€æŸ¥é€šè¿‡')
    except TemplateSyntaxError as e:
        context = _template_context_lines(content, e.lineno, radius=2)
        raise RuntimeError(
            f'æ¨¡æ¿è¯­æ³•é”™è¯¯({template_file}:{e.lineno}, md5:{digest}): {e.message}\né™„è¿‘å†…å®¹:\n{context}'
        )

task_manager = TaskManager()



def smoke_render_pages_on_startup():
    """å¯åŠ¨å‰åšæœ€å°é¡µé¢æ¸²æŸ“å›å½’ï¼Œå°½æ—©å‘ç°æ¨¡æ¿è¿è¡Œæ—¶é—®é¢˜ã€‚"""
    with app.test_request_context('/'):
        render_main_page('semi')
        render_main_page('tasks')
        render_main_page('settings')
    print('âœ… é¡µé¢æ¸²æŸ“å†’çƒŸæ£€æŸ¥é€šè¿‡: /, /tasks, /settings')

def run_scheduler():
    print("ğŸš€ [åå°] ä»»åŠ¡è°ƒåº¦çº¿ç¨‹å·²å¯åŠ¨...")
    while True:
        try:
            schedule.run_pending()
            task_manager.run_refill_scheduler_tick()
        except Exception as e:
            print(f"âš ï¸ è°ƒåº¦æ‰§è¡Œå‡ºé”™: {e}")
            print(traceback.format_exc())
        time.sleep(1)

# å¯åŠ¨åå°çº¿ç¨‹
threading.Thread(target=run_scheduler, daemon=True).start()

# ================= è·¯ç”± =================

@app.route('/')
def index():
    return render_main_page('semi')


def build_dates():
    dates = []
    today = datetime.now()
    weekdays = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"]
    # æ˜¾ç¤ºæœªæ¥ 14 å¤© (2å‘¨) ä»¥æ”¯æŒæ›´è¿œçš„é¢„å®š
    for i in range(14):
        d = today + timedelta(days=i)
        dates.append({
            "val": d.strftime("%Y-%m-%d"),
            "weekday": weekdays[d.weekday()],
            "date_only": d.strftime("%m-%d")
        })
    return dates


def render_main_page(page_mode: str):
    return render_template(
        'index.html',
        dates=build_dates(),
        tasks=task_manager.tasks,
        page_mode=page_mode,
    )


@app.route('/tasks')
@app.route('/tasks/')
def tasks_page():
    return render_main_page('tasks')


@app.route('/settings')
@app.route('/settings/')
def settings_page():
    return render_main_page('settings')

@app.route('/api/matrix')
def api_matrix():
    date = request.args.get('date')
    return jsonify(client.get_matrix(date))

@app.route('/api/mine-overview')
def api_mine_overview():
    orders_res = client.get_place_orders()
    if 'error' in orders_res:
        return jsonify({'error': orders_res.get('error')})
    grouped = client.extract_mine_slots_by_date(orders_res.get('data') or [])
    return jsonify({'records': grouped})


@app.route('/api/time')
def api_time():
    return jsonify({"timestamp": datetime.now().timestamp()})

@app.route('/api/book', methods=['POST'])
def api_book():
    data = request.json
    date = data.get('date')
    items = data.get('items')
    res = client.submit_order(date, items)
    
    # å¢åŠ æ‰‹åŠ¨é¢„è®¢åçš„çŸ­ä¿¡é€šçŸ¥
    # åªè¦çŠ¶æ€ä¸æ˜¯ failï¼Œå°±å‘é€é€šçŸ¥ï¼ˆsuccess æˆ– partialï¼‰
    if res.get('status') in ['success', 'partial']:
        print(f"ğŸ“§ [è°ƒè¯•] å‡†å¤‡å‘é€æ‰‹åŠ¨é¢„è®¢é€šçŸ¥ï¼ŒçŠ¶æ€: {res.get('status')}")
        try:
            status_desc = "å·²é¢„è®¢æˆåŠŸï¼" if res['status'] == 'success' else "å·²é¢„è®¢éƒ¨åˆ†æˆåŠŸï¼"
            detail_msg = f"{status_desc}æ—¥æœŸ{date}: "
            items_str = []
            for item in items:
                items_str.append(f"{item['place']}å·åœº({item['time']})")
            detail_msg += ",".join(items_str)
            detail_msg += "ã€‚"
            
            # å¼ºåˆ¶æ£€æŸ¥ä¸€æ¬¡æ‰‹æœºå·é…ç½®
            phones = CONFIG.get('notification_phones', [])
            if not phones:
                print(f"âš ï¸ [è°ƒè¯•] æ­¤æ—¶å†…å­˜ä¸­ notification_phones ä¸ºç©ºï¼Œå°è¯•é‡æ–°åŠ è½½...")
                if os.path.exists(CONFIG_FILE):
                    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                        saved = json.load(f)
                        CONFIG['notification_phones'] = saved.get('notification_phones', [])
                        print(f"âš ï¸ [è°ƒè¯•] é‡æ–°åŠ è½½åæ‰‹æœºå·: {CONFIG['notification_phones']}")
            
            task_manager.send_notification(detail_msg)
        except Exception as e:
            print(f"æ‰‹åŠ¨é¢„è®¢é€šçŸ¥å‘é€å¤±è´¥: {e}")
            print(traceback.format_exc())
            
    else:
        print(f"ğŸ“§ [è°ƒè¯•] é¢„è®¢çŠ¶æ€ä¸º {res.get('status')}ï¼Œä¸å‘é€é€šçŸ¥ã€‚è¿”å›msg: {res.get('msg')}")
        
    return jsonify(res)

@app.route('/api/config', methods=['GET'])
def get_config():
    return jsonify(CONFIG)

@app.route('/api/config', methods=['POST'])
def update_config():
    """
    æ›´æ–°å…¨å±€é…ç½®ï¼š
    - notification_phonesï¼šå…¨å±€æŠ¥è­¦æ‰‹æœºå·ï¼ˆåˆ—è¡¨ï¼Œå¯ä»¥å¡« 0~N ä¸ªï¼‰
    - pushplus_tokensï¼šå…¨å±€å¾®ä¿¡é€šçŸ¥ tokenï¼ˆåˆ—è¡¨æˆ–é€—å·åˆ†éš”ï¼‰
    - retry_intervalï¼šæ™®é€šé‡è¯•é—´éš”
    - aggressive_retry_intervalï¼šæ­»ç£•æ¨¡å¼é‡è¯•é—´éš”
    - batch_retry_timesï¼šåˆ†æ‰¹å¤±è´¥é‡è¯•æ¬¡æ•°
    - batch_retry_intervalï¼šåˆ†æ‰¹å¤±è´¥é‡è¯•é—´éš”
    - submit_batch_sizeï¼šå•æ‰¹æäº¤ä¸Šé™
    - submit_timeout_secondsï¼šä¸‹å•æ¥å£è¶…æ—¶(ç§’)
    - submit_split_retry_timesï¼šé™çº§åˆ†æ®µé‡è¯•è½®æ¬¡
    - initial_submit_batch_sizeï¼šé¦–æ‰¹æäº¤ä¸Šé™
    - batch_min_intervalï¼šæ‰¹æ¬¡é—´æœ€å°é—´éš”
    - refill_window_secondsï¼šå¤±è´¥åè¡¥æçª—å£
    - locked_retry_intervalï¼šé”å®šçŠ¶æ€é‡è¯•é—´éš”
    - locked_max_secondsï¼šé”å®šçŠ¶æ€æœ€å¤šåˆ· N ç§’
    - open_retry_secondsï¼šå·²å¼€æ”¾æ— ç»„åˆæ—¶ç»§ç»­é‡è¯•çª—å£
    - matrix_timeout_secondsï¼šæŸ¥è¯¢çŸ©é˜µè¶…æ—¶(ç§’)ï¼Œå»ºè®®é«˜å³°æœŸä½¿ç”¨çŸ­è¶…æ—¶
    - order_query_timeout_secondsï¼šè®¢å•æŸ¥è¯¢è¶…æ—¶(ç§’)
    - order_query_max_pagesï¼šè®¢å•æŸ¥è¯¢æœ€å¤§é¡µæ•°
    - post_submit_orders_join_timeout_secondsï¼šæäº¤åè®¢å•æŸ¥è¯¢çº¿ç¨‹ç­‰å¾…ä¸Šé™(ç§’)
    - post_submit_verify_orders_on_matrix_partial_onlyï¼šä»…åœ¨çŸ©é˜µæ ¡éªŒå­˜åœ¨ç¼ºå£æ—¶å†æŸ¥è®¢å•
    - post_submit_orders_sync_fallbackï¼šè®¢å•çº¿ç¨‹è¶…æ—¶åæ˜¯å¦åŒæ­¥å…œåº•
    - post_submit_verify_pending_retry_secondsï¼šéªŒè¯æœªæ”¶æ•›æ—¶å¿«é€Ÿå¤æ ¸é—´éš”(ç§’)
    - post_submit_treat_verify_timeout_as_retryï¼šéªŒè¯è¶…æ—¶æ˜¯å¦èµ°å¿«é€Ÿå¤æ ¸è€Œéç›´æ¥å¤±è´¥
    - stop_on_none_stage_without_refillï¼špipeline é˜¶æ®µç»“æŸä¸”æ—  refill æ—¶ç«‹å³ç»“æŸ
    - health_check_enabled: å¥åº·æ£€æŸ¥æ˜¯å¦å¼€å¯
    - health_check_interval_min: å¥åº·æ£€æŸ¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    - health_check_start_time: å¥åº·æ£€æŸ¥èµ·å§‹æ—¶é—´ï¼ˆHH:MMï¼‰
    - verbose_logs: æ˜¯å¦è¾“å‡ºé«˜é¢‘è°ƒè¯•æ—¥å¿—
    - same_time_precheck_limit: åŒæ—¶æ®µé¢„æ£€ä¸Šé™ï¼ˆ<=0 å…³é—­ï¼‰
    - biz_fail_cooldown_seconds: pipeline ä¸šåŠ¡å¤±è´¥å†·å´ç§’æ•°
    - preselect_enabledï¼šæ˜¯å¦å¯ç”¨è§£é”å‰é¢„é€‰å¿«ç…§
    - preselect_ttl_secondsï¼šé¢„é€‰å¿«ç…§æœ‰æ•ˆæœŸ(ç§’)
    - preselect_only_before_first_submitï¼šä»…é¦–æå‰å¯ç”¨é¢„é€‰å¿«ç…§
    """
    try:
        data = request.json or {}

        # è¯»å–æ—§é…ç½®ï¼Œä¿è¯ auth / sms ç­‰å­—æ®µä¸ä¼šä¸¢
        saved = {}
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    saved = json.load(f) or {}
            except Exception as e:
                print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
                saved = {}

        # ç¡®ä¿ auth / sms ç»“æ„å­˜åœ¨ï¼ˆä¸æ”¹åŠ¨å®ƒä»¬ï¼‰
        if 'auth' not in saved:
            saved['auth'] = CONFIG.get('auth', {}).copy()
        if 'sms' not in saved:
            saved['sms'] = CONFIG.get('sms', {}).copy()

        # å°å·¥å…·ï¼šæ›´æ–°ä¸€ä¸ªæµ®ç‚¹å­—æ®µï¼ˆå¸¦æœ€å°å€¼ä¸é»˜è®¤å€¼ï¼‰
        def _update_float_field(field, min_value, default_value):
            if field not in data:
                return
            try:
                val = float(data[field])
            except (TypeError, ValueError):
                val = default_value
            if val < min_value:
                val = min_value
            CONFIG[field] = val
            saved[field] = val

        # 1) å…¨å±€æŠ¥è­¦æ‰‹æœºå·
        if 'notification_phones' in data:
            phones = data['notification_phones'] or []
            if isinstance(phones, str):
                phones = [p.strip() for p in phones.split(',') if p.strip()]
            elif isinstance(phones, list):
                phones = [str(p).strip() for p in phones if str(p).strip()]
            else:
                phones = []
            CONFIG['notification_phones'] = phones
            saved['notification_phones'] = phones

        # 1.1) å…¨å±€å¾®ä¿¡é€šçŸ¥ tokenï¼ˆPushPlusï¼‰
        if 'pushplus_tokens' in data:
            tokens = data['pushplus_tokens'] or []
            if isinstance(tokens, str):
                tokens = [t.strip() for t in tokens.split(',') if t.strip()]
            elif isinstance(tokens, list):
                tokens = [str(t).strip() for t in tokens if str(t).strip()]
            else:
                tokens = []
            CONFIG['pushplus_tokens'] = tokens
            saved['pushplus_tokens'] = tokens

        # 2) å„ç±»é‡è¯• / é™åˆ¶é…ç½®
        _update_float_field('retry_interval', 0.1, CONFIG.get('retry_interval', 1.0))
        _update_float_field('aggressive_retry_interval', 0.1, CONFIG.get('aggressive_retry_interval', 0.3))
        _update_float_field('batch_retry_interval', 0.1, CONFIG.get('batch_retry_interval', 0.5))
        _update_float_field('submit_timeout_seconds', 0.5, CONFIG.get('submit_timeout_seconds', 4.0))
        _update_float_field('batch_min_interval', 0.1, CONFIG.get('batch_min_interval', 0.8))
        _update_float_field('refill_window_seconds', 0.0, CONFIG.get('refill_window_seconds', 8.0))
        _update_float_field('locked_retry_interval', 0.1, CONFIG.get('locked_retry_interval', 1.0))
        _update_float_field('locked_max_seconds', 1.0, CONFIG.get('locked_max_seconds', 60.0))
        _update_float_field('open_retry_seconds', 0.0, CONFIG.get('open_retry_seconds', 30.0))
        _update_float_field('matrix_timeout_seconds', 0.5, CONFIG.get('matrix_timeout_seconds', 3.0))
        _update_float_field('order_query_timeout_seconds', 0.5, CONFIG.get('order_query_timeout_seconds', 2.5))
        _update_float_field('post_submit_orders_join_timeout_seconds', 0.1, CONFIG.get('post_submit_orders_join_timeout_seconds', 1.2))
        _update_float_field('post_submit_verify_pending_retry_seconds', 0.05, CONFIG.get('post_submit_verify_pending_retry_seconds', 0.35))
        _update_float_field('health_check_interval_min', 1.0, CONFIG.get('health_check_interval_min', 30.0))
        _update_float_field('biz_fail_cooldown_seconds', 1.0, CONFIG.get('biz_fail_cooldown_seconds', 15.0))
        _update_float_field('preselect_ttl_seconds', 0.2, CONFIG.get('preselect_ttl_seconds', 2.0))

        if 'post_submit_verify_orders_on_matrix_partial_only' in data:
            val = data['post_submit_verify_orders_on_matrix_partial_only']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['post_submit_verify_orders_on_matrix_partial_only'] = enabled
            saved['post_submit_verify_orders_on_matrix_partial_only'] = enabled

        if 'post_submit_orders_sync_fallback' in data:
            val = data['post_submit_orders_sync_fallback']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['post_submit_orders_sync_fallback'] = enabled
            saved['post_submit_orders_sync_fallback'] = enabled

        if 'post_submit_treat_verify_timeout_as_retry' in data:
            val = data['post_submit_treat_verify_timeout_as_retry']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['post_submit_treat_verify_timeout_as_retry'] = enabled
            saved['post_submit_treat_verify_timeout_as_retry'] = enabled

        if 'stop_on_none_stage_without_refill' in data:
            val = data['stop_on_none_stage_without_refill']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['stop_on_none_stage_without_refill'] = enabled
            saved['stop_on_none_stage_without_refill'] = enabled

        if 'batch_retry_times' in data:
            try:
                val = int(data['batch_retry_times'])
            except (TypeError, ValueError):
                val = int(CONFIG.get('batch_retry_times', 2))
            val = max(0, min(5, val))
            CONFIG['batch_retry_times'] = val
            saved['batch_retry_times'] = val

        if 'submit_batch_size' in data:
            try:
                val = int(data['submit_batch_size'])
            except (TypeError, ValueError):
                val = int(CONFIG.get('submit_batch_size', 3))
            val = max(1, min(9, val))
            CONFIG['submit_batch_size'] = val
            saved['submit_batch_size'] = val

        if 'initial_submit_batch_size' in data:
            try:
                val = int(data['initial_submit_batch_size'])
            except (TypeError, ValueError):
                val = int(CONFIG.get('initial_submit_batch_size', CONFIG.get('submit_batch_size', 3)))
            val = max(1, min(9, val))
            CONFIG['initial_submit_batch_size'] = val
            saved['initial_submit_batch_size'] = val

        if 'order_query_max_pages' in data:
            try:
                val = int(data['order_query_max_pages'])
            except (TypeError, ValueError):
                val = int(CONFIG.get('order_query_max_pages', 2))
            val = max(1, min(10, val))
            CONFIG['order_query_max_pages'] = val
            saved['order_query_max_pages'] = val

        if 'submit_split_retry_times' in data:
            try:
                val = int(data['submit_split_retry_times'])
            except (TypeError, ValueError):
                val = int(CONFIG.get('submit_split_retry_times', 1))
            val = max(0, min(3, val))
            CONFIG['submit_split_retry_times'] = val
            saved['submit_split_retry_times'] = val

        if 'health_check_start_time' in data:
            time_str = normalize_time_str(data['health_check_start_time'])
            if time_str:
                CONFIG['health_check_start_time'] = time_str
                saved['health_check_start_time'] = time_str

        # 3) å¥åº·æ£€æŸ¥å¼€å…³ï¼ˆå‹¾é€‰ / å–æ¶ˆï¼‰
        if 'preselect_enabled' in data:
            val = data['preselect_enabled']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['preselect_enabled'] = enabled
            saved['preselect_enabled'] = enabled

        if 'preselect_only_before_first_submit' in data:
            val = data['preselect_only_before_first_submit']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['preselect_only_before_first_submit'] = enabled
            saved['preselect_only_before_first_submit'] = enabled

        if 'health_check_enabled' in data:
            val = data['health_check_enabled']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['health_check_enabled'] = enabled
            saved['health_check_enabled'] = enabled

        # 3.1) é«˜é¢‘è°ƒè¯•æ—¥å¿—å¼€å…³
        if 'verbose_logs' in data:
            val = data['verbose_logs']
            if isinstance(val, bool):
                enabled = val
            elif isinstance(val, str):
                enabled = val.lower() in ('1', 'true', 'yes', 'on')
            else:
                enabled = bool(val)
            CONFIG['verbose_logs'] = enabled
            saved['verbose_logs'] = enabled

        # 3.2) åŒæ—¶æ®µé¢„æ£€ä¸Šé™ï¼ˆ<=0 è¡¨ç¤ºå…³é—­ï¼‰
        if 'same_time_precheck_limit' in data:
            try:
                val = int(data.get('same_time_precheck_limit'))
            except (TypeError, ValueError):
                val = int(CONFIG.get('same_time_precheck_limit', 0))
            val = max(0, min(9, val))
            CONFIG['same_time_precheck_limit'] = val
            saved['same_time_precheck_limit'] = val

        # 4) å†™å› config.json
        try:
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(saved, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"å†™å…¥é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            # å³ä½¿å†™æ–‡ä»¶å¤±è´¥ï¼Œå†…å­˜ä¸­çš„ CONFIG å·²ç»æ›´æ–°äº†

        # 5) é‡æ–°å®‰æ’å¥åº·æ£€æŸ¥ï¼ˆåº”ç”¨æ–°çš„å¼€å…³/é—´éš”ï¼‰
        schedule_health_check()

        return jsonify({"status": "success"})

    except Exception as e:
        print(f"æ›´æ–°é…ç½®æ—¶å¼‚å¸¸: {e}")
        return jsonify({"status": "error", "msg": str(e)})


@app.route('/api/config/auth', methods=['POST'])
def update_auth():
    try:
        data = request.json
        if not data:
            return jsonify({"status": "error", "msg": "è¯·æ±‚ä½“ä¸ºç©º"})
            
        token = str(data.get('token') or '').strip()
        if not token:
            return jsonify({"status": "error", "msg": "Tokenç¼ºå¤±"})

        cookie_raw = data.get('cookie', None)
        cookie = str(cookie_raw).strip() if cookie_raw is not None else ''
        has_cookie_update = bool(cookie)

        CONFIG['auth']['token'] = token
        if has_cookie_update:
            CONFIG['auth']['cookie'] = cookie

        # æ›´æ–° client å®ä¾‹
        client.token = token
        if has_cookie_update:
            client.headers['Cookie'] = cookie
            
            # æŒä¹…åŒ–ä¿å­˜
            try:
                saved = {}
                if os.path.exists(CONFIG_FILE):
                    try:
                        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                            saved = json.load(f)
                    except: pass
                
                # ç¡®ä¿ auth ç»“æ„å­˜åœ¨
                if 'auth' not in saved: saved['auth'] = {}
                
                saved['auth']['token'] = token
                if has_cookie_update:
                    saved['auth']['cookie'] = cookie
                else:
                    saved['auth']['cookie'] = CONFIG['auth'].get('cookie', '')
                # ä¿ç•™å…¶ä»– auth å­—æ®µ (å¦‚ shop_num)
                saved['auth']['card_index'] = CONFIG['auth'].get('card_index', '')
                saved['auth']['card_st_id'] = CONFIG['auth'].get('card_st_id', '')
                saved['auth']['shop_num'] = CONFIG['auth'].get('shop_num', '')

                with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                    json.dump(saved, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                print(f"ä¿å­˜Authé…ç½®å¤±è´¥: {e}")
                # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œå†…å­˜æ›´æ–°æˆåŠŸä¹Ÿç®—æˆåŠŸï¼Œä½†è®°å½•æ—¥å¿—

            if has_cookie_update:
                msg = "Token/Cookie å·²æ›´æ–°"
            else:
                msg = "Token å·²æ›´æ–°ï¼ŒCookie ä¿æŒåŸå€¼"
            return jsonify({"status": "success", "msg": msg})
    except Exception as e:
        print(f"Update Auth Error: {e}")
        return jsonify({"status": "error", "msg": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"})

@app.route('/api/tasks', methods=['GET'])
def get_tasks():
    return jsonify(task_manager.tasks)

@app.route('/api/tasks', methods=['POST'])
def add_task():
    data = request.json
    task_manager.add_task(data)
    return jsonify({"status": "success"})

@app.route('/api/tasks/<task_id>', methods=['DELETE'])
def del_task(task_id):
    task_manager.delete_task(task_id)
    return jsonify({"status": "success"})

@app.route('/api/tasks/<task_id>', methods=['PUT'])
def update_task(task_id):
    data = request.json or {}
    ok = task_manager.update_task(task_id, data)
    if not ok:
        return jsonify({"status": "error", "msg": "Task not found"}), 404
    return jsonify({"status": "success"})

@app.route('/api/tasks/<task_id>/run', methods=['POST'])
def run_task_now(task_id):
    # Find task
    task = next((t for t in task_manager.tasks if str(t['id']) == str(task_id)), None)
    if task:
        if task_manager.is_task_running(task_id):
            return jsonify({"status": "error", "msg": "ä»»åŠ¡ä»åœ¨æ‰§è¡Œä¸­ï¼Œæœ¬æ¬¡è§¦å‘å·²è·³è¿‡"}), 409
        # Run in a separate thread to avoid blocking the response
        threading.Thread(target=task_manager.execute_task_with_lock, args=(task,)).start()
        return jsonify({"status": "success", "msg": "Task started"})
    return jsonify({"status": "error", "msg": "Task not found"}), 404



@app.route('/api/state-sampler', methods=['GET'])
def api_state_sampler():
    snap = STATE_SAMPLER.snapshot()
    return jsonify({
        'status': 'success',
        'seconds': snap.get('seconds', 0),
        'states': snap.get('states', {}),
        'recommended_locked_states': snap.get('recommended_locked_states', []),
        'current_locked_state_values': CONFIG.get('locked_state_values', []),
    })


@app.route('/api/refill-tasks', methods=['GET'])
def get_refill_tasks():
    return jsonify(task_manager.refill_tasks)


@app.route('/api/refill-tasks', methods=['POST'])
def add_refill_task_api():
    data = request.json or {}
    task = task_manager.add_refill_task(data)
    return jsonify({'status': 'success', 'task': task})




@app.route('/api/refill-tasks/<task_id>', methods=['PUT'])
def update_refill_task_api(task_id):
    data = request.json or {}
    task = task_manager.update_refill_task(task_id, data)
    if not task:
        return jsonify({'status': 'error', 'msg': 'Refill task not found'}), 404
    return jsonify({'status': 'success', 'task': task})

@app.route('/api/refill-tasks/<task_id>', methods=['DELETE'])
def del_refill_task_api(task_id):
    task_manager.delete_refill_task(task_id)
    return jsonify({'status': 'success'})


@app.route('/api/refill-tasks/<task_id>/run', methods=['POST'])
def run_refill_task_now(task_id):
    task = next((t for t in task_manager.refill_tasks if str(t.get('id')) == str(task_id)), None)
    if not task:
        return jsonify({'status': 'error', 'msg': 'Refill task not found'}), 404

    task['last_result'] = {'status': 'running', 'msg': 'æ‰‹åŠ¨æ‰§è¡Œä¸­(1è½®)'}
    task_manager.save_refill_tasks()

    def _run():
        try:
            res = task_manager._run_refill_task_once(task, source='manual')
            task['last_run_at'] = int(time.time() * 1000)
            task['last_result'] = res
            task_manager.append_refill_history(task, res)
            task_manager.save_refill_tasks()
        except Exception as e:
            task['last_run_at'] = int(time.time() * 1000)
            task['last_result'] = {'status': 'error', 'msg': str(e)}
            task_manager.append_refill_history(task, task['last_result'])
            task_manager.save_refill_tasks()

    threading.Thread(target=_run, daemon=True).start()
    return jsonify({'status': 'success', 'msg': 'Refill task one-shot started'})

@app.route('/api/config/check-token', methods=['POST'])
def check_token_api():
    query_ok, query_msg = client.check_token()
    booking_probe = client.check_booking_auth_probe()

    if query_ok:
        status = "success"
        msg = "æŸ¥è¯¢é“¾è·¯é‰´æƒé€šè¿‡ã€‚"
    else:
        status = "error"
        msg = f"æŸ¥è¯¢é“¾è·¯é‰´æƒå¤±è´¥: {query_msg}"
        # å¦‚æœå¤±æ•ˆï¼Œå°è¯•å‘çŸ­ä¿¡æé†’ï¼ˆå¦‚æœé…ç½®äº†æ‰‹æœºå·ï¼‰
        task_manager.send_notification(f"è­¦å‘Šï¼šæ‚¨çš„ Token å¯èƒ½å·²å¤±æ•ˆ ({query_msg})ï¼Œè¯·åŠæ—¶æ›´æ–°å–µï¼")

    return jsonify({
        "status": status,
        "msg": msg,
        "query_auth_ok": query_ok,
        "query_auth_msg": query_msg,
        "booking_auth_ok": booking_probe.get('ok', False),
        "booking_auth_unknown": booking_probe.get('unknown', True),
        "booking_auth_msg": booking_probe.get('msg', ''),
    })

@app.route('/api/config/test-sms', methods=['POST'])
def test_sms():
    data = request.json
    phones = data.get('phones', [])
    if not phones: return jsonify({"status": "error", "msg": "è¯·è¾“å…¥æ‰‹æœºå·å–µ"})
    
    # ä¸´æ—¶è¦†ç›–é…ç½®ä»¥æµ‹è¯•
    original_phones = CONFIG.get('notification_phones', [])
    CONFIG['notification_phones'] = phones
    
    try:
        # å°è¯•å‘é€
        success, msg = task_manager.send_notification("è¿™æ˜¯ä¸€æ¡æµ‹è¯•çŸ­ä¿¡ï¼Œæ”¶åˆ°ä»£è¡¨é…ç½®æˆåŠŸå–µï¼")
        if success:
            return jsonify({"status": "success", "msg": "æ¥å£è°ƒç”¨æˆåŠŸ(è¿”å›ç 0)ï¼Œè¯·ç•™æ„æ‰‹æœºçŸ­ä¿¡å–µ"})
        else:
            return jsonify({"status": "error", "msg": f"å‘é€å¤±è´¥: {msg} å–µ"})
    except Exception as e:
        print(f"æµ‹è¯•æ¥å£å¼‚å¸¸: {e}")
        return jsonify({"status": "error", "msg": f"æœåŠ¡ç«¯å¼‚å¸¸: {str(e)}"})
    finally:
        # æ¢å¤é…ç½®
        CONFIG['notification_phones'] = original_phones



@app.route('/<path:path_like>')
def page_route_fallback(path_like):
    # reverse proxy / sub-path compatibility: support /xxx/tasks or /xxx/settings
    normalized = (path_like or '').strip('/')
    if not normalized:
        return render_main_page('semi')

    # keep API/static 404 behavior
    if normalized.startswith('api/') or normalized.startswith('static/'):
        return jsonify({"status": "error", "msg": "Not Found"}), 404

    last = normalized.split('/')[-1]
    if last in ('tasks', 'settings'):
        return render_main_page(last)
    if last in ('', 'index', 'semi'):
        return render_main_page('semi')

    return jsonify({"status": "error", "msg": "Not Found"}), 404

@app.route('/api/logs', methods=['GET'])
def get_logs():
    refill_id = (request.args.get('refill_id') or '').strip()
    status_kw = (request.args.get('status_kw') or '').strip().lower()
    try:
        window_min = max(0, int(float(request.args.get('window_min', 0) or 0)))
    except Exception:
        window_min = 0

    logs = list(LOG_BUFFER)
    if window_min > 0:
        now_dt = datetime.now()
        cutoff = now_dt - timedelta(minutes=window_min)
        filtered = []
        for line in logs:
            try:
                if len(line) >= 10 and line[0] == '[' and line[9] == ']':
                    t = datetime.strptime(line[1:9], '%H:%M:%S')
                    cur = now_dt.replace(hour=t.hour, minute=t.minute, second=t.second, microsecond=0)
                    if cur > now_dt:
                        cur = cur - timedelta(days=1)
                    if cur >= cutoff:
                        filtered.append(line)
                else:
                    filtered.append(line)
            except Exception:
                filtered.append(line)
        logs = filtered
    if refill_id:
        key = f"[refill#{refill_id}|"
        logs = [line for line in logs if key in line]
    if status_kw:
        logs = [line for line in logs if status_kw in str(line).lower()]
    return jsonify(logs)


@app.route('/api/run-metrics', methods=['GET'])
def get_run_metrics():
    task_id = request.args.get('task_id')
    unlock_only = str(request.args.get('unlock_only', '1')).lower() in ('1', 'true', 'yes', 'on')
    limit = request.args.get('limit', default=50, type=int)
    limit = max(1, min(500, int(limit or 50)))
    records = []
    if os.path.exists(TASK_RUN_METRICS_FILE):
        try:
            with open(TASK_RUN_METRICS_FILE, 'r', encoding='utf-8') as f:
                records = json.load(f) or []
        except Exception:
            records = []
    if not isinstance(records, list):
        records = []
    if task_id:
        records = [r for r in records if str(r.get('task_id')) == str(task_id)]
    if unlock_only:
        records = [
            r for r in records
            if bool(r.get('saw_locked')) and bool(r.get('unlocked_after_locked'))
        ]
    records = records[-limit:]

    success_within_60 = [r for r in records if r.get('success_within_60s') is True]
    first_success_samples = sorted(int(r.get('first_success_ms')) for r in records if r.get('first_success_ms') is not None)
    submit_p95_samples = sorted(int(r.get('submit_latency_p95_ms')) for r in records if r.get('submit_latency_p95_ms') is not None)
    summary = {
        'total_runs': len(records),
        'unlock_only': unlock_only,
        'focus_scope': 'locked_to_unlocked_only' if unlock_only else 'all_runs',
        'success_within_60_rate': round(len(success_within_60) / len(records), 4) if records else None,
        'first_success_p50_ms': int(_percentile(first_success_samples, 0.5)) if first_success_samples else None,
        'first_success_p95_ms': int(_percentile(first_success_samples, 0.95)) if first_success_samples else None,
        'submit_p95_p50_ms': int(_percentile(submit_p95_samples, 0.5)) if submit_p95_samples else None,
        'goal_achieved_rate': round(sum(1 for r in records if bool(r.get('goal_achieved'))) / len(records), 4) if records else None,
    }

    recommendation = {
        'profile': 'balanced',
        'reason': 'æ•°æ®ä¸è¶³ï¼Œå…ˆç”¨å¹³è¡¡æ¡£æŒç»­é‡‡æ ·',
    }
    rate = summary.get('success_within_60_rate')
    first_p95 = summary.get('first_success_p95_ms')
    submit_p95_med = summary.get('submit_p95_p50_ms')
    min_sample_size = 12
    confidence = 'low'
    if records:
        if len(records) >= min_sample_size and rate is not None and first_p95 is not None and submit_p95_med is not None:
            confidence = 'high'
            if rate >= 0.6 and first_p95 <= 12000 and submit_p95_med <= 2500:
                recommendation = {'profile': 'stable', 'reason': 'å‘½ä¸­ç‡é«˜ä¸”æ—¶å»¶ç¨³å®šï¼Œå»ºè®®ç¨³å¥æ¡£é™ä½é£æ§é£é™©'}
            elif rate < 0.35 or first_p95 > 25000 or submit_p95_med > 4500:
                recommendation = {'profile': 'aggressive', 'reason': '60ç§’å‘½ä¸­ç‡åä½æˆ–æ—¶å»¶åé«˜ï¼Œå»ºè®®æ¿€è¿›æ¡£æå‡å‰60ç§’å‘½ä¸­'}
            else:
                recommendation = {'profile': 'balanced', 'reason': 'å‘½ä¸­ç‡ä¸æ—¶å»¶å±…ä¸­ï¼Œå»ºè®®å¹³è¡¡æ¡£æŒç»­è§‚å¯Ÿ'}
        else:
            recommendation = {'profile': 'balanced', 'reason': f'æ ·æœ¬ä¸è¶³ï¼ˆ{len(records)}/{min_sample_size}ï¼‰ï¼Œå…ˆä¿æŒå¹³è¡¡æ¡£å¹¶ç»§ç»­é‡‡æ ·'}
    recommendation['confidence'] = confidence
    recommendation['sample_size'] = len(records)
    recommendation['min_sample_size'] = min_sample_size
    return jsonify({'summary': summary, 'recommendation': recommendation, 'records': records})

if __name__ == "__main__":
    validate_templates_on_startup()
    smoke_render_pages_on_startup()

    # é¦–æ¬¡å¯åŠ¨åˆ·æ–°è°ƒåº¦
    task_manager.refresh_schedule()

    # å¯åŠ¨å¥åº·æ£€æŸ¥è°ƒåº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    schedule_health_check()

    print("ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼Œè®¿é—® http://127.0.0.1:5000")
    print("ğŸ“‹ å·²åŠ è½½æµ‹è¯•æ¥å£: /api/config/test-sms")
    app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)  # å…³é—­ reloader é˜²æ­¢çº¿ç¨‹é‡å¤å¯åŠ¨
